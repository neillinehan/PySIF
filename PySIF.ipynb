{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#install"
      ],
      "metadata": {
        "id": "t2tHp4knbOxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycwt"
      ],
      "metadata": {
        "id": "LpfkbjTtbQ3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ace2097-e587-4734-b5f5-d50ad0aea8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycwt\n",
            "  Downloading pycwt-0.4.0b0-py3-none-any.whl (753 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.5/753.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2,>=1.24 (from pycwt)\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pycwt) (1.11.3)\n",
            "Requirement already satisfied: matplotlib<4,>=3.7 in /usr/local/lib/python3.10/dist-packages (from pycwt) (3.7.1)\n",
            "Requirement already satisfied: tqdm<5,>=4.65 in /usr/local/lib/python3.10/dist-packages (from pycwt) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4,>=3.7->pycwt) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.7->pycwt) (1.16.0)\n",
            "Installing collected packages: numpy, pycwt\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.1 pycwt-0.4.0b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAdfv6nbsHP3"
      },
      "source": [
        "#imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "ALPHA = string.ascii_letters\n",
        "import math\n",
        "import matplotlib.colors\n",
        "import time\n",
        "import matplotlib.dates as mdates\n",
        "import re\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from scipy.signal import find_peaks, peak_widths, peak_prominences\n",
        "import seaborn as sns\n",
        "\n",
        "from typing import List\n",
        "import gc\n",
        "\n",
        "from matplotlib.image import NonUniformImage\n",
        "from matplotlib.ticker import ScalarFormatter, FuncFormatter\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pycwt\n",
        "import pycwt as wavelet\n",
        "from pycwt.helpers import (ar1,get_cache_dir, rednoise)\n",
        "from pycwt.mothers import DOG, MexicanHat, Morlet, Paul\n",
        "from multiprocessing import Pool\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import matplotlib.patches as mpatches\n"
      ],
      "metadata": {
        "id": "MAQFWSM9BQ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULuE3z8dntPY"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Functions"
      ],
      "metadata": {
        "id": "Ybhzjz-k1GmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###General"
      ],
      "metadata": {
        "id": "eU5unpg94DqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-UICwYHF3u"
      },
      "outputs": [],
      "source": [
        "def create_SIF_dataframe(text_files: List[str], start_date: str = '2023-05-14', end_date: str = '2023', timezone: str = 'US/Central') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a DataFrame for the Solar-Induced Fluorescence (SIF) data as well as other indices computed with the SIF software GUI.\n",
        "    Also adds some key environmental variables such as clearness index, Vapour-pressure deficit, and PRI scaled.\n",
        "\n",
        "    Parameters:\n",
        "        text_files (List[str]): List of text files containing SIF data.\n",
        "        start_date (str): The start date for the data range. Defaults to '2023-05-14'.\n",
        "        end_date (str): The end date for the data range. Defaults to '2023'.\n",
        "        timezone (str): The timezone for the data. Defaults to 'US/Central'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with SIF data.\n",
        "    \"\"\"\n",
        "    # Read the SIF data from the text file\n",
        "    SIF_data_df = pd.read_csv([file for file in text_files if 'ALL_INDEX' in file][0], sep=';', na_values=[r'#N/D'])\n",
        "\n",
        "    # Convert the datetime column to datetime format and adjust for duplicates\n",
        "    SIF_data_df['datetime [UTC]'] = pd.to_datetime(SIF_data_df['datetime [UTC]'], utc=True)\n",
        "    SIF_data_df['datetime [UTC]'] += pd.to_timedelta(SIF_data_df.groupby('datetime [UTC]').cumcount(), unit='m')\n",
        "\n",
        "    # Convert UTC to local timezone and set as index\n",
        "    SIF_data_df.index = SIF_data_df['datetime [UTC]'].dt.tz_convert(timezone)\n",
        "    SIF_data_df.index.name = 'datetime'\n",
        "\n",
        "    # Filter the DataFrame based on the specified date range\n",
        "    SIF_data_df = SIF_data_df.sort_index().loc[start_date: end_date]\n",
        "\n",
        "    # Calculate additional columns\n",
        "    SIF_data_df['PRI_s'] = ((SIF_data_df['PRI'] + 1 ) / 2)\n",
        "    VPD = (1 - SIF_data_df['h2 [%]']) * .611 * np.exp((17.27 * SIF_data_df['temp4 [C]'])  / (237.3 + SIF_data_df['temp4 [C]']))\n",
        "    SIF_data_df['VPD'] = VPD\n",
        "    Rg_vis = SIF_data_df['PAR inc [W m-2]']\n",
        "    R0_vis = 1367 * (1 + .033 * np.cos(2 * np.pi * SIF_data_df['doy.dayfract'] / 365)) * np.cos(SIF_data_df['SZA'] * (np.pi / 180)) * .389\n",
        "    CI = (Rg_vis / R0_vis).to_frame('CI')\n",
        "    SIF_data_df['CI'] = CI\n",
        "\n",
        "    return SIF_data_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_timeseries_to_day_col(dataframe: pd.DataFrame, column_names: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reshapes a time series DataFrame into a DataFrame where time is the index and dates are the columns.\n",
        "    This is useful for conducting analysis such as looking at the trend of every day at a specific time.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame.\n",
        "        column_names (List[str]): The names of the columns to be processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The processed DataFrame with time as the index and dates as the columns.\n",
        "    \"\"\"\n",
        "    # Copy the specified columns from the input DataFrame\n",
        "    processed_df = dataframe[column_names].copy()\n",
        "\n",
        "    # Change the index to time and date\n",
        "    processed_df.index = [processed_df.index.time, processed_df.index.date]\n",
        "\n",
        "    # Unstack the DataFrame (this pivots the DataFrame from long to wide format)\n",
        "    processed_df = processed_df.unstack()\n",
        "\n",
        "    # Interpolate the DataFrame\n",
        "    processed_df = processed_df.interpolate()\n",
        "\n",
        "    return processed_df"
      ],
      "metadata": {
        "id": "XaCOubQOqvxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_multiscale_rolling_correlations(dataframe: pd.DataFrame, column1: str, column2: str, start: int, end: int, steps: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates rolling correlations between two signals in a DataFrame\n",
        "    at different window sizes, similar to a wavelet transform. This allows for\n",
        "    cross-temporal scale analysis.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame.\n",
        "        column1, column2 (str): The names of the two columns to calculate correlations between.\n",
        "        start, end (int): The start and end values for the window sizes in seconds.\n",
        "        steps (int): The number of steps between start and end.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the rolling correlations for each window size.\n",
        "    \"\"\"\n",
        "    # Generate a list of window sizes in a geometric sequence\n",
        "    window_sizes = np.char.add(np.geomspace(start, end, steps).astype(int).astype(str), 's')\n",
        "\n",
        "    corr_dfs = []\n",
        "    # For each window size...\n",
        "    for window in window_sizes:\n",
        "        # Calculate the rolling correlation between the two columns\n",
        "        rolling_df = dataframe[[column1, column2]]\n",
        "        corr_df = rolling_df[column1].rolling(window=window, center=True).corr(rolling_df[column2])\n",
        "        corr_dfs.append(corr_df)\n",
        "\n",
        "    # Combine the DataFrames for each window size into a single DataFrame\n",
        "    corr_dfs_var_win_len = pd.concat(corr_dfs, axis=1)\n",
        "    corr_dfs_var_win_len.columns = np.geomspace(start, end, steps).astype(int)\n",
        "\n",
        "    return corr_dfs_var_win_len"
      ],
      "metadata": {
        "id": "rBl78gUEvACC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-esTg2Mjt79"
      },
      "outputs": [],
      "source": [
        "def filter_text_files(text_files: List[str], *conditions: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    This function takes in a list of text files and any number of conditions,\n",
        "    and returns the filtered text files based on the conditions.\n",
        "\n",
        "    :param text_files: List of text files to filter\n",
        "    :param conditions: Any number of conditions to filter the text files\n",
        "    :return: A list of filtered text files\n",
        "    \"\"\"\n",
        "    # Create a list to store the filtered text files\n",
        "    filtered_text_files = []\n",
        "\n",
        "    # Loop over each text file\n",
        "    for f in text_files:\n",
        "        # Assume that the file meets all conditions\n",
        "        meets_all_conditions = True\n",
        "\n",
        "        # Loop over each condition\n",
        "        for condition in conditions:\n",
        "            # Check if the condition is not blank\n",
        "            if condition:\n",
        "                # Check if the condition is not met by the file\n",
        "                if condition not in f:\n",
        "                    # Set the flag to False and break out of the loop\n",
        "                    meets_all_conditions = False\n",
        "                    break\n",
        "\n",
        "        # Check if the file meets all conditions\n",
        "        if meets_all_conditions:\n",
        "            # Add the file to the list of filtered text files\n",
        "            filtered_text_files.append(f)\n",
        "\n",
        "    # Return the list of filtered text files\n",
        "    return filtered_text_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcdyNMfuUhaH"
      },
      "outputs": [],
      "source": [
        "def process_raw_spectrometer_file(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes a raw spectrometer file and returns a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the raw spectrometer file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the processed data.\n",
        "    \"\"\"\n",
        "    # Read raw data from CSV file\n",
        "    raw_data = pd.read_csv(file_path, header=None, low_memory=True)\n",
        "\n",
        "    # Extract header data\n",
        "    header_data = raw_data.loc[raw_data[0].str[0].str.isdigit(), :][0].str.split(';', expand=True)\n",
        "\n",
        "    # Parse timestamp from header data\n",
        "    date_time = pd.to_datetime(header_data[1].astype(str) + header_data[2].astype(str), format='%y%m%d%H%M%S').rename('date-time')\n",
        "    date_time.index += 1\n",
        "\n",
        "    # Split data arrays\n",
        "    array_data = raw_data[0].str.split(';', expand=True)\n",
        "    data_arrays = array_data[array_data[0].str.startswith(tuple(ALPHA))]\n",
        "\n",
        "    # Combine timestamp and data arrays\n",
        "    fluo_array_df = pd.concat([date_time, data_arrays], axis=1).sort_index()\n",
        "    fluo_array_df.iloc[:, 0].fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Reshape data arrays to create rows for each cycle\n",
        "    fluo_array_df = fluo_array_df.set_index(['date-time', 0]).T\n",
        "    fluo_array_df.drop(fluo_array_df.tail(1).index, inplace=True)\n",
        "\n",
        "    # Set column names\n",
        "    fluo_array_df.columns.rename({0: 'array'}, inplace=True)\n",
        "    fluo_array_df = fluo_array_df.T\n",
        "    fluo_array_df = fluo_array_df.apply(pd.to_numeric, errors='ignore', downcast='integer')\n",
        "\n",
        "    return fluo_array_df\n",
        "\n",
        "\n",
        "def process_raw_spectrometer_files(file_paths: List[str], output_directory: str):\n",
        "    \"\"\"\n",
        "    Processes multiple raw spectrometer files and saves the results to CSV files.\n",
        "\n",
        "    Parameters:\n",
        "        file_paths (List[str]): List of file paths for the raw spectrometer files.\n",
        "        output_directory (str): The directory to save the processed CSV files.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        # Process the file\n",
        "        result = process_raw_spectrometer_file(file_path)\n",
        "\n",
        "        # Generate a unique file name\n",
        "        output_file = f\"{output_directory}/processed_file_{i}.csv\"\n",
        "\n",
        "        # Save the result to a CSV file\n",
        "        result.to_csv(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqOTEeVkblIQ"
      },
      "outputs": [],
      "source": [
        "def combine_processed_raw_spectrometer_files(file_paths):\n",
        "    \"\"\"\n",
        "    Combines multiple processed raw spectrometer files into a single DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    file_paths (list): List of file paths for the spectrometer files to combine.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing the combined data from all input files.\n",
        "    \"\"\"\n",
        "    dataframe_list = []\n",
        "    for file_path in file_paths:\n",
        "        dataframe = pd.read_csv(file_path, index_col=[0, 1], header=[0])\n",
        "        dataframe_list.append(dataframe)\n",
        "    combined_dataframe = pd.concat(dataframe_list)\n",
        "    return combined_dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def red_noise_fill_gaps(series):\n",
        "    # Check if the series is 'doy.dayfract'\n",
        "    if series.name == 'doy.dayfract':\n",
        "        # Resample the series for every minute\n",
        "        series_resampled = series.resample('1T').mean()\n",
        "\n",
        "        # Create a fractional day of the year series\n",
        "        doy = series_resampled.index.dayofyear\n",
        "        day_fraction = series_resampled.index.hour / 24 + series_resampled.index.minute / (24 * 60) + series_resampled.index.second / (24 * 60 * 60)\n",
        "        fractional_doy = doy + day_fraction\n",
        "\n",
        "        return fractional_doy\n",
        "    else:\n",
        "        # Calculate al1 on the original data\n",
        "        signal = series.dropna().values\n",
        "        al1 = ar1(signal)[0]\n",
        "\n",
        "        # Resample the series for every minute\n",
        "        series_resampled = series.resample('1T').mean()\n",
        "\n",
        "        # Linearly interpolate gaps that are 1-2 minute wide\n",
        "        series_interpolated = series_resampled.interpolate(method='linear', limit=2)\n",
        "\n",
        "        # Generate red noise for NaN gaps\n",
        "        N = len(series_interpolated)\n",
        "        noise1 = rednoise(N, al1, 1)\n",
        "\n",
        "        # Substitute red noise for NaN gaps in the resampled data\n",
        "        series_filled = series_interpolated.where(~np.isnan(series_interpolated), noise1)\n",
        "\n",
        "        return series_filled"
      ],
      "metadata": {
        "id": "w7IY4u8uFrEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g_UUwCFd7_F"
      },
      "outputs": [],
      "source": [
        "def df_from_processed_spectrometer_files(text_files, condition1, condition2):\n",
        "    \"\"\"\n",
        "    This function takes in a list of text files, and two conditions to filter the files.\n",
        "    It reads the data from the filtered text files into a pandas DataFrame, and performs\n",
        "    some processing on the data to create a final DataFrame.\n",
        "\n",
        "    Used to process radiance and reflectance data.\n",
        "\n",
        "    :param text_files: List of text files to read data from\n",
        "    :param condition1 (str): First condition to filter the text files\n",
        "    :param condition2 (str): Second condition to filter the text files\n",
        "    :return: A pandas DataFrame containing the processed data\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter the text files based on the given conditions\n",
        "    filtered_text_files = filter_text_files(text_files, condition1, condition2)\n",
        "\n",
        "    # Read the data from the filtered text files into a DataFrame\n",
        "    df = pd.concat((pd.read_csv(f, sep=';', header=[0], index_col=[0],na_values=[r'#N/D']).T\n",
        "                   .assign(date=f.rsplit('/',2)[-2]) # add column with date info from filename\n",
        "           for f in filtered_text_files),\n",
        "                             axis=0)\n",
        "\n",
        "    # Convert the index to a datetime object representing time\n",
        "    df['time'] = pd.to_datetime(df.index,\n",
        "               format='%H_%M_%S',\n",
        "               utc=True,\n",
        "               exact=False,\n",
        "               infer_datetime_format=False,\n",
        "               )\n",
        "    # Create a new column 'date_time' by combining the 'date' and 'time' columns\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%y%m%d', utc=False)\n",
        "    df['date_time'] = df['date'] + pd.to_timedelta(df['time'].dt.tz_convert('US/Central').dt.strftime('%H:%M:%S'))\n",
        "    # Add time delta to 'date_time' column to handle duplicate values\n",
        "    df['date_time'] += pd.to_timedelta(df.groupby('date_time').cumcount(), unit='m')\n",
        "\n",
        "    # Set the index of the DataFrame to the 'date_time' column and rename it to 'datetime'\n",
        "    df.index = df['date_time']\n",
        "    df.index.name = 'datetime'\n",
        "\n",
        "    # Drop unnecessary columns and sort the DataFrame by index\n",
        "    df = df.drop(['date','time','date_time'],axis=1).sort_index()\n",
        "\n",
        "    # Localize the index to the given timezone\n",
        "    df.index= df.index.tz_localize('US/Central')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIxcF0aJIjW4"
      },
      "outputs": [],
      "source": [
        "def check_for_skipped_proccessed_spectrometer_files(up_text_files, path):\n",
        "    \"\"\"\n",
        "    This function takes in a list of text files and a path to a directory containing CSV files.\n",
        "    It processes the text files and CSV files to create several lists of file paths.\n",
        "\n",
        "    :param up_text_files: List of text files to process\n",
        "    :param path: Path to a directory containing CSV files\n",
        "    :return: A tuple containing several lists of file paths\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter the text files based on their size and whether they contain certain substrings\n",
        "    files_over_100 = [f for f in up_text_files if 'nightlog' not in f and '/F' not in f and os.path.getsize(f) > (1024*100) if f]\n",
        "\n",
        "    # Extract the date and filename from the filtered text files\n",
        "    raw_files = [f.rsplit('/',2)[-2] + \"/\" + f.rsplit('/',1)[-1].rsplit('.')[0] for f in files_over_100]\n",
        "\n",
        "    # Get a list of all CSV files in the given path\n",
        "    text_files = glob.glob(path + \"/**/*.csv\", recursive = True)\n",
        "\n",
        "    # Filter the CSV files based on whether they contain certain substrings\n",
        "    processed_files = [f for f in text_files if 'Incoming' in f and 'FLUO' in f ]\n",
        "\n",
        "    # Extract the date and filename from the filtered CSV files\n",
        "    proccesed_files = [f.rsplit('/',2)[-2] + \"/\" + f.rsplit('_',1)[-1].split('.')[0] for f in processed_files]\n",
        "\n",
        "    # Find the raw files that were not processed\n",
        "    skipped_files = [f for f in raw_files if f not in proccesed_files]\n",
        "\n",
        "    # Find the paths of the skipped files\n",
        "    skipped_paths = [f for f in files_over_100 for f2 in skipped_files if f2 in f]\n",
        "    print(len(skipped_paths),'skipped files')\n",
        "    if len(skipped_paths) > 0:\n",
        "      return skipped_paths\n",
        "    else:\n",
        "      return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn3TU-O9YsCg"
      },
      "outputs": [],
      "source": [
        "drop_indexs_FLUO = [4, 6, 8, 10, 12, 14, 16, 18, 20, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51]\n",
        "drop_indexs_FULL = [4, 6, 8, 10, 12, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33]\n",
        "drop_indexs_NL = [2, 4, 6, 8, 10, 12]\n",
        "\n",
        "rename_dict_FLUO = {5: 'IT_WR[us]=',\n",
        " 7: 'IT_VEG[us]=',\n",
        " 9: 'cycle_duration[ms]=',\n",
        " 11: 'QEpro_Frame[C]=',\n",
        " 13: 'QEpro_CCD[C]=',\n",
        " 15: 'chamber_temp[C]=',\n",
        " 17: 'chamber_humidity=',\n",
        " 19: 'mainboard_temp[C]=',\n",
        " 21: 'mainboard_humidity=',\n",
        " 24: 'GPS_TIME_UTC=',\n",
        " 26: 'GPS_date=',\n",
        " 28: 'GPS_lat=',\n",
        " 30: 'GPS_lon=',\n",
        " 32: 'GPS_alt=',\n",
        " 34: 'GPS_prec=',\n",
        " 36: 'voltage=',\n",
        " 38: 'gps_CPU=',\n",
        " 40: 'wr_CPU=',\n",
        " 42: 'veg_CPU=',\n",
        " 44: 'cooling_active=',\n",
        " 46: 'heating_active=',\n",
        " 48: 'Temp0',\n",
        " 50: 'Temp1',\n",
        " 52: 'Temp2',\n",
        " 0: 'Cycle Number',\n",
        " 1: 'Date',\n",
        " 2: 'Time',\n",
        " 3: 'Mode'}\n",
        "\n",
        "rename_dict_FULL = {5: 'IT_WR[us]=',\n",
        " 7: 'IT_VEG[us]=',\n",
        " 9: 'cycle_duration[ms]=',\n",
        " 11: 'mainboard_temp[C]=',\n",
        " 13: 'mainboard_humidity=',\n",
        " 16: 'GPS_TIME_UTC=',\n",
        " 18: 'GPS_date=',\n",
        " 20: 'GPS_lat=',\n",
        " 22: 'GPS_lon=',\n",
        " 24: 'GPS_alt=',\n",
        " 26: 'GPS_prec=',\n",
        " 28: 'voltage=',\n",
        " 30: 'gps_CPU=',\n",
        " 32: 'wr_CPU=',\n",
        " 34: 'veg_CPU=',\n",
        " 0: 'Cycle Number',\n",
        " 1: 'Date',\n",
        " 2: 'Time',\n",
        " 3: 'Mode'}\n",
        "\n",
        "rename_dict_NL = {3: 'mainboard_temp[C]=',\n",
        " 5: 'chamber_temp[C]=',\n",
        " 7: 'mainboard_humidity=',\n",
        " 9: 'chamber_humidity=',\n",
        " 11: 'voltage=',\n",
        " 13: 'On at=',\n",
        " 0: 'Date',\n",
        " 1: 'Time',}\n",
        "\n",
        "rename_dicts_dict = {'FLUO':rename_dict_FLUO,\n",
        "                'FULL':rename_dict_FULL,\n",
        "                'nightlog':rename_dict_NL}\n",
        "drop_index_dict = {'FLUO':drop_indexs_FLUO,\n",
        "                'FULL':drop_indexs_FULL,\n",
        "                'nightlog':drop_indexs_NL}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1432OZbkQ6Bk"
      },
      "outputs": [],
      "source": [
        "def process_raw_spectrometer_sensor_data(text_files, file_type):\n",
        "    \"\"\"\n",
        "    Processes raw sensor data from spectrometer files.\n",
        "\n",
        "    :param text_files: List of text files containing raw sensor data\n",
        "    :param file_type: Type of file to process. Can be 'FLUO', 'FULL', or 'nightlog'\n",
        "    :return: DataFrame containing processed sensor data\n",
        "    \"\"\"\n",
        "    # Filter files based on file type\n",
        "    if file_type == 'FLUO':\n",
        "        files = [f for f in text_files if 'nightlog' not in f and '/F' not in f]\n",
        "    elif file_type == 'FULL':\n",
        "        files = [f for f in text_files if 'nightlog' not in f and '/F' in f]\n",
        "    elif file_type == 'nightlog':\n",
        "        files = [f for f in text_files if 'nightlog' in f]\n",
        "\n",
        "    # Read and concatenate data from all files\n",
        "    df = pd.concat([\n",
        "        pd.read_csv(f, header=None).assign(Filename=f.split('/')[-2] + '/' + f.split('/')[-1])\n",
        "        for f in files\n",
        "    ])\n",
        "\n",
        "    # Filter rows and split data into columns\n",
        "    df = df[~df.iloc[:, 0].str.startswith(('W', 'D', 'V'))]\n",
        "    df.loc[:, list(range(54))] = df[0].str.split(';', expand=True)\n",
        "\n",
        "    # Rename and drop columns based on file type\n",
        "    df = df.rename(columns=rename_dicts_dict[file_type]).drop(columns=drop_index_dict[file_type])\n",
        "\n",
        "    # Drop rows containing the pattern\n",
        "    pattern = r'[=]'\n",
        "    mask = df.stack().str.contains(pattern).groupby(level=0).any()\n",
        "    df = df[~mask]\n",
        "\n",
        "    # Convert time column to datetime object\n",
        "    df['Time'] = pd.to_datetime(\n",
        "        df.Time,\n",
        "        format='%H%M%S',\n",
        "        utc=False,\n",
        "        exact=False,\n",
        "        infer_datetime_format=False,\n",
        "    )\n",
        "\n",
        "    # Combine date and time columns into a single datetime column\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%y%m%d', utc=False)\n",
        "    df['date_time'] = df['Date'] + pd.to_timedelta(\n",
        "        df['Time'].dt.tz_localize('US/Central').dt.strftime('%H:%M:%S')\n",
        "    )\n",
        "\n",
        "    # Add time delta to handle duplicate values\n",
        "    df['date_time'] += pd.to_timedelta(df.groupby('date_time').cumcount(), unit='m')\n",
        "\n",
        "    # Set the index to the datetime column and localize to the given timezone\n",
        "    df.index = df['date_time']\n",
        "    df.index.name = 'datetime'\n",
        "    df.index = df.index.tz_localize('US/Central')\n",
        "\n",
        "    # Drop unnecessary columns and sort the DataFrame by index\n",
        "    df = df.drop(['Date', 'Time', 'date_time'], axis=1).sort_index()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYkkEOONfzJZ"
      },
      "outputs": [],
      "source": [
        "def convert_seconds_to_labels(seconds):\n",
        "    \"\"\"\n",
        "    Converts a list of seconds into a list of time labels in seconds, minutes, hours, or days.\n",
        "\n",
        "    Parameters:\n",
        "    seconds (list): The list of seconds to convert.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of time labels.\n",
        "    \"\"\"\n",
        "    time_labels = []\n",
        "    for time_in_seconds in seconds:\n",
        "        if time_in_seconds < 60:\n",
        "            time_labels.append(f\"{time_in_seconds} s\")\n",
        "        elif 60 <= time_in_seconds < 3600:\n",
        "            time_labels.append(f\"{np.round(time_in_seconds / 60)} mins\")\n",
        "        elif 3600 <= time_in_seconds < 86400:\n",
        "            time_labels.append(f\"{np.round(time_in_seconds / 3600)} hours\")\n",
        "        elif time_in_seconds >= 86400:\n",
        "            time_labels.append(f\"{np.round(time_in_seconds / 86400)} days\")\n",
        "    return time_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goViq9DbiZ7P"
      },
      "outputs": [],
      "source": [
        "def get_peak_info(row):\n",
        "    \"\"\"\n",
        "    This function takes in a row of data and returns the Full Width at Half Maximum (FWHM), peak wavelength, and spectral resolution.\n",
        "\n",
        "    Parameters:\n",
        "        row (pd.Series): A row of data containing the values and index.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the FWHM, peak wavelength, and spectral resolution.\n",
        "    \"\"\"\n",
        "    # Flatten the values and negate them\n",
        "    x = -row.values.flatten()\n",
        "\n",
        "    # Get the index values\n",
        "    y = row.index.values\n",
        "\n",
        "    # Find the peaks\n",
        "    peaks, _ = find_peaks(x)\n",
        "\n",
        "    # Get the index of the maximum peak prominence\n",
        "    max_i = peak_prominences(x, peaks)[0].argmax()\n",
        "\n",
        "    # Calculate the peak widths\n",
        "    widths = peak_widths(x, peaks)\n",
        "\n",
        "    # Calculate the spectral resolution\n",
        "    spec_res = np.diff(y).mean()\n",
        "\n",
        "    # Calculate the Full Width at Half Maximum (FWHM)\n",
        "    FWHM = np.round(widths[0][max_i] * spec_res, 3)\n",
        "\n",
        "    # Calculate the peak wavelength\n",
        "    peak_wl = np.round(y[peaks[max_i]], 3)\n",
        "\n",
        "    return [FWHM, peak_wl, spec_res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfTdFtFrNXa5"
      },
      "outputs": [],
      "source": [
        "def reclassify_values_to_colors(values, colors):\n",
        "    \"\"\"\n",
        "    Reclassifies values into categories based on their range and assigns a color to each category.\n",
        "    For rolling-correlation function.\n",
        "\n",
        "    Parameters:\n",
        "    values (np.array): The array of values to reclassify.\n",
        "    colors (list): The list of colors to assign to each category.\n",
        "\n",
        "    Returns:\n",
        "    np.array: An array of the same shape as values with the reclassified colors.\n",
        "    \"\"\"\n",
        "    # Create an array of the same shape as values filled with the default color\n",
        "    result = np.full(values.shape, '#ffffff')\n",
        "\n",
        "    # Reclassify the values based on their range and assign the corresponding color\n",
        "    result[values < -0.6] = colors[0]\n",
        "    result[(values >= -0.6) & (values < -0.2)] = colors[1]\n",
        "    result[(values >= -0.2) & (values < 0.2)] = colors[2]\n",
        "    result[(values >= 0.2) & (values < 0.6)] = colors[3]\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_gaps_within_days(dataframe, timezone='America/Chicago'):\n",
        "    \"\"\"\n",
        "    Fills gaps within days in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The DataFrame to process.\n",
        "    timezone (str): The timezone for the data. Defaults to 'America/Chicago'.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame with gaps filled within days.\n",
        "    \"\"\"\n",
        "    # Copy the DataFrame\n",
        "    resampled_df = dataframe.copy()\n",
        "\n",
        "    # Change the index to time and date\n",
        "    resampled_df.index = [resampled_df.index.time, resampled_df.index.date]\n",
        "\n",
        "    # Unstack the DataFrame, interpolate missing values, and stack it back\n",
        "    unstacked_df = resampled_df.unstack().interpolate(limit_area='inside').stack(dropna=False)\n",
        "\n",
        "    # Convert the multi-index back to a single datetime index\n",
        "    unstacked_df.index = pd.to_datetime(unstacked_df.index.get_level_values(0).astype(str) + ' ' + unstacked_df.index.get_level_values(1).astype(str))\n",
        "\n",
        "    # Make the datetime index timezone aware\n",
        "    unstacked_df.index = unstacked_df.index.tz_localize(timezone)\n",
        "\n",
        "    return unstacked_df.sort_index()\n"
      ],
      "metadata": {
        "id": "oFqdNvxO16Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_timeseries_into_days(dataframe):\n",
        "    \"\"\"\n",
        "    Splits a time series DataFrame into a DataFrame where time is the index and dates are the columns.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The processed DataFrame with time as the index and dates as the columns.\n",
        "    \"\"\"\n",
        "    # Copy the input DataFrame\n",
        "    resampled_df = dataframe.copy()\n",
        "\n",
        "    # Change the index to time and date\n",
        "    resampled_df.index = [resampled_df.index.time, resampled_df.index.date]\n",
        "\n",
        "    # Unstack the DataFrame (this pivots the DataFrame from long to wide format)\n",
        "    unstacked_df = resampled_df.unstack()\n",
        "\n",
        "    return unstacked_df\n"
      ],
      "metadata": {
        "id": "hptK1WAl2CRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_regular_data_chunks(dataframe, min_length=40):\n",
        "    \"\"\"\n",
        "    Analyzes regular data chunks in a DataFrame and prints information about them.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The DataFrame to analyze.\n",
        "    min_length (int): The minimum length of a chunk to consider. Defaults to 40.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Sort the DataFrame by index\n",
        "    dataframe = dataframe.sort_index()\n",
        "\n",
        "    # Create a boolean series that is True where your DataFrame has NaN values\n",
        "    isna = dataframe.isna().any(axis=1)\n",
        "\n",
        "    # Create groups for each continuous chunk of data\n",
        "    groups = isna.ne(isna.shift()).cumsum()\n",
        "\n",
        "    chunks = []\n",
        "    for _, chunk in dataframe.groupby(groups):\n",
        "        if not chunk.isna().any().any() and len(chunk) >= min_length:\n",
        "            # Process the chunk here and append it to the list\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    # Calculate the average length of the chunks\n",
        "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
        "    min_length = min(len(chunk) for chunk in chunks)\n",
        "    max_length = max(len(chunk) for chunk in chunks)\n",
        "\n",
        "    # Print the statistics\n",
        "    print(f\"Average length of the chunks: {avg_length}\")\n",
        "    print(f\"Smallest chunk length: {min_length}\")\n",
        "    print(f\"Largest chunk length: {max_length}\")\n",
        "    print(f\"Total number of chunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "sPGdUf0HJGM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clipped_values(series, lower_quantile=0.05, upper_quantile=0.95):\n",
        "    \"\"\"\n",
        "    Returns the indices and values of the clipped data points for a given series.\n",
        "\n",
        "    Parameters:\n",
        "    series (pd.Series): The input data series.\n",
        "    lower_quantile (float): The lower quantile for clipping. Defaults to 0.05.\n",
        "    upper_quantile (float): The upper quantile for clipping. Defaults to 0.95.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of two tuples, each containing the indices and values of the clipped data points below and above the quantiles.\n",
        "    \"\"\"\n",
        "    # Get the lower and upper clipping thresholds\n",
        "    lower_threshold = series.quantile(lower_quantile)\n",
        "    upper_threshold = series.quantile(upper_quantile)\n",
        "\n",
        "    # Get the indices and values of the clipped data points below and above the thresholds\n",
        "    clipped_below = (series.index[series < lower_threshold], series[series < lower_threshold])\n",
        "    clipped_above = (series.index[series > upper_threshold], series[series > upper_threshold])\n",
        "\n",
        "    # Return a list of tuples\n",
        "    return [clipped_below, clipped_above]"
      ],
      "metadata": {
        "id": "efnGqcO1z8ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Wavelet Coherence"
      ],
      "metadata": {
        "id": "ufjngEnS3wPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wct(\n",
        "    y1,\n",
        "    y2,\n",
        "    dt,\n",
        "    dj=1 / 12,\n",
        "    s0=-1,\n",
        "    J=-1,\n",
        "    sig=True,\n",
        "    significance_level=0.95,\n",
        "    wavelet_type=\"morlet\",\n",
        "    normalize=True,\n",
        "    partial = True,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"Wavelet coherence transform (WCT).\n",
        "\n",
        "    The WCT finds regions in time frequency space where the two time\n",
        "    series co-vary, but do not necessarily have high power.\n",
        "\n",
        "    Adapted from PYCWT pacakge.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y1, y2 : numpy.ndarray, list\n",
        "        Input signals.\n",
        "    dt : float\n",
        "        Sample spacing.\n",
        "    dj : float, optional\n",
        "        Spacing between discrete scales. Default value is 1/12.\n",
        "        Smaller values will result in better scale resolution, but\n",
        "        slower calculation and plot.\n",
        "    s0 : float, optional\n",
        "        Smallest scale of the wavelet. Default value is 2*dt.\n",
        "    J : float, optional\n",
        "        Number of scales less one. Scales range from s0 up to\n",
        "        s0 * 2**(J * dj), which gives a total of (J + 1) scales.\n",
        "        Default is J = (log2(N*dt/so))/dj.\n",
        "    sig : bool\n",
        "        set to compute signficance, default is True\n",
        "    significance_level (float, optional) :\n",
        "        Significance level to use. Default is 0.95.\n",
        "    normalize (boolean, optional) :\n",
        "        If set to true, normalizes CWT by the standard deviation of\n",
        "        the signals.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WCT : magnitude of coherence\n",
        "    aWCT : phase angle of coherence\n",
        "    coi (array like):\n",
        "        Cone of influence, which is a vector of N points containing\n",
        "        the maximum Fourier period of useful information at that\n",
        "        particular time. Periods greater than those are subject to\n",
        "        edge effects.\n",
        "    freq (array like):\n",
        "        Vector of Fourier equivalent frequencies (in 1 / time units)    coi :\n",
        "    sig :  Significance levels as a function of scale\n",
        "       if sig=True when called, otherwise zero.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "    cwt, xwt\n",
        "\n",
        "    \"\"\"\n",
        "    wavelet_type = _check_parameter_wavelet(wavelet_type)\n",
        "\n",
        "    # Checking some input parameters\n",
        "    if s0 == -1:\n",
        "        # Number of scales\n",
        "        s0 = 2 * dt / wavelet_type.flambda()\n",
        "    if J == -1:\n",
        "        # Number of scales\n",
        "        J = int(np.round(np.log2(y1.size * dt / s0) / dj))\n",
        "\n",
        "    # Makes sure input signals are numpy arrays.\n",
        "    y1 = np.asarray(y1)\n",
        "    y2 = np.asarray(y2)\n",
        "    # Calculates the standard deviation of both input signals.\n",
        "    std1 = y1.std()\n",
        "    std2 = y2.std()\n",
        "    # Normalizes both signals, if appropriate.\n",
        "    if normalize:\n",
        "        y1_normal = (y1 - y1.mean()) / std1\n",
        "        y2_normal = (y2 - y2.mean()) / std2\n",
        "    else:\n",
        "        y1_normal = y1\n",
        "        y2_normal = y2\n",
        "\n",
        "    # Calculates the CWT of the time-series making sure the same parameters\n",
        "    # are used in both calculations.\n",
        "    _kwargs = dict(dj=dj, s0=s0, J=J, wavelet=wavelet_type)\n",
        "    W1, sj, freq, coi, _, _ = wavelet.cwt(y1_normal, dt, **_kwargs)\n",
        "    W2, sj, freq, coi, _, _ = wavelet.cwt(y2_normal, dt, **_kwargs)\n",
        "\n",
        "    scales1 = np.ones([1, y1.size]) * sj[:, None]\n",
        "    scales2 = np.ones([1, y2.size]) * sj[:, None]\n",
        "\n",
        "    # Smooth the wavelet spectra before truncating.\n",
        "    S1 = wavelet_type.smooth(np.abs(W1) ** 2 / scales1, dt, dj, sj)\n",
        "    S2 = wavelet_type.smooth(np.abs(W2) ** 2 / scales2, dt, dj, sj)\n",
        "\n",
        "    # Now the wavelet transform coherence\n",
        "    W12 = W1 * W2.conj()\n",
        "    scales = np.ones([1, y1.size]) * sj[:, None]\n",
        "    S12 = wavelet_type.smooth(W12 / scales, dt, dj, sj)\n",
        "    R = S12 / np.sqrt(S1 * S2)\n",
        "    R2 = np.abs(R) ** 2\n",
        "    WCT = R2\n",
        "    aWCT = np.angle(W12)\n",
        "\n",
        "    # Calculates the significance using Monte Carlo simulations with 95%\n",
        "    # confidence as a function of scale.\n",
        "    if sig:\n",
        "        a1, b1, c1 = ar1(y1)\n",
        "        a2, b2, c2 = ar1(y2)\n",
        "\n",
        "        sig = wct_significance(\n",
        "            a1,\n",
        "            a2,\n",
        "            dt=dt,\n",
        "            dj=dj,\n",
        "            s0=s0,\n",
        "            J=J,\n",
        "            significance_level=significance_level,\n",
        "            wavelet_type=wavelet_type,\n",
        "            **kwargs,\n",
        "        )\n",
        "    else:\n",
        "        sig = np.asarray([0])\n",
        "\n",
        "    return WCT, aWCT, coi, freq, sig,R"
      ],
      "metadata": {
        "id": "osJIF6pabn7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_WCT_time_lag(frequencies: np.ndarray, cone_of_influence: np.ndarray, significance: np.ndarray, wavelet_coherence_angle: np.ndarray) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculate the time lag and phase angle based on the given parameters.\n",
        "\n",
        "    Parameters:\n",
        "    frequencies (np.ndarray): Frequency array.\n",
        "    cone_of_influence (np.ndarray): Cone of influence array.\n",
        "    significance (np.ndarray): Significance array.\n",
        "    wavelet_coherence_angle (np.ndarray): Cross wavelet transform array.\n",
        "\n",
        "    Returns:\n",
        "    time_lag_df (pd.DataFrame): DataFrame containing various calculated values.\n",
        "    phase_df (pd.DataFrame): DataFrame containing phase values.\n",
        "\n",
        "    References:\n",
        "    - https://sites.google.com/a/glaciology.net/grinsted/wavelet-coherence/faq\n",
        "    - Application of the cross wavelet transform and wavelet coherence to geophysical time series\n",
        "    \"\"\"\n",
        "    # Calculate the inside of the cone of influence and periods in hours\n",
        "    inside_coi = np.outer(1/frequencies, 1./cone_of_influence) > 1\n",
        "    periods_hours = 1/frequencies / (60*60)\n",
        "\n",
        "    # Calculate the phase where significance is greater than or equal to 1\n",
        "    phase = np.where(significance>=1,wavelet_coherence_angle,np.nan)\n",
        "    phase_in_coi = np.where(~inside_coi,phase,np.nan)\n",
        "\n",
        "    # Create a DataFrame for the phase and drop rows with all NaN values\n",
        "    phase_df = pd.DataFrame(phase_in_coi)\n",
        "    phase_df.index = periods_hours\n",
        "    phase_df.index.name = 'period (hours)'\n",
        "    phase_df.dropna(how='all',axis=0,inplace=True)\n",
        "\n",
        "    # Calculate the circular mean and standard deviation\n",
        "    X = np.cos(phase_df).sum(axis=1)\n",
        "    Y = np.sin(phase_df).sum(axis=1)\n",
        "    R = np.sqrt(X**2+Y**2)\n",
        "    circular_mean = np.arctan2(Y,X)*180/np.pi\n",
        "    count = phase_df.count(axis=1)\n",
        "    circular_std = np.sqrt(-2 * np.log(R/count))*180/np.pi\n",
        "\n",
        "    # Calculate the mean and standard deviation of the time lag\n",
        "    time_lag_mean = (circular_mean/360 * phase_df.index).rename('mean_time_lag')\n",
        "    time_lag_std = (circular_std/360 * phase_df.index).rename('std_time_lag')\n",
        "\n",
        "    # Create a DataFrame for the time lag\n",
        "    time_lag_df = pd.concat([time_lag_mean,\n",
        "                             time_lag_std,\n",
        "                             circular_mean.rename('mean_angle'),\n",
        "                             circular_std.rename('std_angle'),\n",
        "                             count.rename('num_data_points')],axis=1)\n",
        "\n",
        "    # Adjust phase values in phase_df\n",
        "    phase_df[phase_df < 0] += 2 * np.pi\n",
        "\n",
        "    # Calculate the circular mean and standard deviation for all data points in phase_df\n",
        "    X = np.cos(phase_df).sum().sum()\n",
        "    Y = np.sin(phase_df).sum().sum()\n",
        "    R = np.sqrt(X**2+Y**2)\n",
        "    circular_mean_all = np.arctan2(Y,X)*180/np.pi\n",
        "    count_all = count.sum()\n",
        "    circular_std_all = np.sqrt(-2 * np.log(R/count_all))*180/np.pi\n",
        "\n",
        "    print(f'average phase angle: {circular_mean_all} +- {circular_std_all}')\n",
        "\n",
        "    return time_lag_df, phase_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HA8oE77if0i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l3144fVwitn"
      },
      "outputs": [],
      "source": [
        "def calc_wave_coherence(wave1, wave2, dt, min_period=60, max_period=3600, sig=False,normalize=True, scales_per_octave=12):\n",
        "    \"\"\"\n",
        "    Calculate wavelet coherence between wave1 and wave2 using pycwt.\n",
        "    modified source:\n",
        "    https://seankmartin.github.io/Claustrum_Experiment/html/bvmpc/lfp_coherence.html#bvmpc.lfp_coherence.calc_coherence\n",
        "    Parameters\n",
        "    ----------\n",
        "    wave1 : np.ndarray\n",
        "        The values of the first waveform.\n",
        "    wave2 : np.ndarray\n",
        "        The values of the second waveform.\n",
        "    dt : float\n",
        "        Time in seconds between data values.\n",
        "    min_period : float\n",
        "        minimum temporal scale to look at wave coherence, in seconds.\n",
        "    max_period : float\n",
        "        maximum temporal scale to look at wave coherence, in seconds.\n",
        "    sig : bool, default False\n",
        "        Optional Should significance of waveform coherence be calculated.\n",
        "    scales_per_octave : int\n",
        "        How many scale subdivions between each octave. Octave: each doubling of\n",
        "        min_period\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WCT, t, freq, coi, sig, aWCT\n",
        "        WCT - 2D numpy array with coherence values\n",
        "        t - 2D numpy array with sample_times\n",
        "        freq - 1D numpy array with the frequencies wavelets were calculated at\n",
        "        coi - 1D numpy array with a frequency value for each time\n",
        "        sig - 2D numpy array indicating where data is significant by monte carlo\n",
        "        aWCT - 2D numpy array with same shape as aWCT indicating phase angles\n",
        "    \"\"\"\n",
        "    # Define the wavelet and sampling period\n",
        "    wavelet_type = _check_parameter_wavelet(\"morlet\")\n",
        "\n",
        "    n = len(wave1)\n",
        "    if min_period < (2 * dt):\n",
        "        min_period = 2 * dt\n",
        "    min_scale = min_period/wavelet_type.flambda()\n",
        "    max_scale = max_period/wavelet_type.flambda()\n",
        "    if max_period == -1:\n",
        "      J = -1\n",
        "    else:\n",
        "      num_octaves = np.log2(max_scale / min_scale)\n",
        "      num_scales = math.ceil(np.log2(max_scale / min_scale) * scales_per_octave)\n",
        "      J = num_scales #- 1\n",
        "      print('number of scales:',num_scales)\n",
        "    dj = 1.0/scales_per_octave\n",
        "    # Do the actual calculation\n",
        "    print(\"Calculating coherence...\")\n",
        "    start_time = time.time()\n",
        "    WCT, aWCT, coi, freq, sig,R = wct(\n",
        "        wave1, wave2, dt,  # Fixed params\n",
        "        dj=dj, s0=min_scale, J=J, sig=sig, normalize=True)\n",
        "    if len(sig) != 1:\n",
        "      sig = WCT/sig[:, None]\n",
        "    print(\"Time Taken: %s s\" % (time.time() - start_time))\n",
        "    if np.max(WCT) > 1 or np.min(WCT) < 0:\n",
        "        print('WCT was out of range: min {},max {}'.format(\n",
        "            np.min(WCT), np.max(WCT)))\n",
        "        WCT = np.clip(WCT, 0, 1)\n",
        "\n",
        "    return WCT, freq, coi, sig, aWCT,R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_partial_wave_coherence(x, y, z,dt, min_period=60,\n",
        "                                max_period=3600, sig=False,normalize=True,\n",
        "                                scales_per_octave=12,significance_level=.95):\n",
        "    \"\"\"\n",
        "    Calculate wavelet coherence between wave1 and wave2 using pycwt.\n",
        "    modified source:\n",
        "    https://seankmartin.github.io/Claustrum_Experiment/html/bvmpc/lfp_coherence.html#bvmpc.lfp_coherence.calc_coherence\n",
        "    Parameters\n",
        "    ----------\n",
        "    wave1 : np.ndarray\n",
        "        The values of the first waveform.\n",
        "    wave2 : np.ndarray\n",
        "        The values of the second waveform.\n",
        "    dt : float\n",
        "        Time in seconds between data values.\n",
        "    min_period : float\n",
        "        minimum temporal scale to look at wave coherence, in seconds.\n",
        "    max_period : float\n",
        "        maximum temporal scale to look at wave coherence, in seconds.\n",
        "    sig : bool, default False\n",
        "        Optional Should significance of waveform coherence be calculated.\n",
        "    scales_per_octave : int\n",
        "        How many scale subdivions between each octave. Octave: each doubling of\n",
        "        min_period\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WCT, t, freq, coi, sig, aWCT\n",
        "        WCT - 2D numpy array with coherence values\n",
        "        t - 2D numpy array with sample_times\n",
        "        freq - 1D numpy array with the frequencies wavelets were calculated at\n",
        "        coi - 1D numpy array with a frequency value for each time\n",
        "        sig - 2D numpy array indicating where data is significant by monte carlo\n",
        "        aWCT - 2D numpy array with same shape as aWCT indicating phase angles\n",
        "    \"\"\"\n",
        "    # Define the wavelet and sampling period\n",
        "    wavelet_type = _check_parameter_wavelet(\"morlet\")\n",
        "\n",
        "    n = len(x)\n",
        "    if min_period < (2 * dt):\n",
        "        min_period = 2 * dt\n",
        "    min_scale = min_period/wavelet_type.flambda()\n",
        "    max_scale = max_period/wavelet_type.flambda()\n",
        "    if max_period == -1:\n",
        "      J = -1\n",
        "    else:\n",
        "      num_octaves = np.log2(max_scale / min_scale)\n",
        "      num_scales = math.ceil(np.log2(max_scale / min_scale) * scales_per_octave)\n",
        "      J = num_scales #- 1\n",
        "      print('number of scales:',num_scales)\n",
        "    dj = 1.0/scales_per_octave\n",
        "    # Do the actual calculation\n",
        "    print(\"Calculating partial coherence...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    _, _, coi, freq, _,Rxy = wct(\n",
        "        x, y, dt,  # Fixed params\n",
        "        dj=dj, s0=min_scale, J=J, sig=False, normalize=True)\n",
        "    print('Rxy done')\n",
        "    _, _, _, _, _,Rxz = wct(\n",
        "        x, z, dt,  # Fixed params\n",
        "        dj=dj, s0=min_scale, J=J, sig=False, normalize=True)\n",
        "    print('Rxz done')\n",
        "    _, _, _, _, _,Ryz = wct(\n",
        "        y, z, dt,  # Fixed params\n",
        "        dj=dj, s0=min_scale, J=J, sig=False, normalize=True)\n",
        "    print('Ryz done')\n",
        "\n",
        "    RPxyz = ( Rxy - Rxz * Ryz.conj() ) / np.sqrt( 1 - abs(Rxz)**2) / np.sqrt( 1 - abs(Ryz)**2)\n",
        "    pWCT = abs(RPxyz)**2\n",
        "    apWCT = np.angle(RPxyz)\n",
        "\n",
        "\n",
        "\n",
        "    if sig:\n",
        "      alx,_,_ = ar1(x)\n",
        "      aly,_,_ = ar1(y)\n",
        "\n",
        "      sig_scales = wct_significance(alx,aly,\n",
        "                                           dt=dt,dj=dj,s0=min_scale,J=J,\n",
        "            significance_level=significance_level,\n",
        "            wavelet_type=wavelet_type,)\n",
        "      print('signifigance finished')\n",
        "      sig_matrix = pWCT/sig_scales[:, None]\n",
        "    else:\n",
        "      sig_matrix = [0]\n",
        "    print(\"Time Taken: %s s\" % (time.time() - start_time))\n",
        "\n",
        "    if np.max(pWCT) > 1 or np.min(pWCT) < 0:\n",
        "        print('pWCT was out of range: min {},max {}'.format(\n",
        "            np.min(pWCT), np.max(pWCT)))\n",
        "        pWCT = np.clip(pWCT, 0, 1)\n",
        "\n",
        "    return pWCT, freq, coi, sig_matrix, apWCT"
      ],
      "metadata": {
        "id": "gYKUErZqs_iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coherence_to_df(WCT, freq, coi, sig, aWCT, index):\n",
        "    # Convert freq to period\n",
        "    period = 1 / freq\n",
        "\n",
        "    # Create a DataFrame with timestamp as index and period as columns for WCT and aWCT\n",
        "    WCT_df = pd.DataFrame(WCT.T, index=index, columns=period)\n",
        "    aWCT_df = pd.DataFrame(aWCT.T, index=index, columns=period)\n",
        "\n",
        "    # For coi and sig, create a DataFrame with timestamp as index\n",
        "    coi_df = pd.DataFrame(coi, index=index)\n",
        "\n",
        "    # For sig, create a DataFrame with timestamp as index and period as columns if it's not [0]\n",
        "    if (not (np.array(sig) == 0).all()) and (sig is not None):\n",
        "        sig_df = pd.DataFrame(sig.T, index=index, columns=period)\n",
        "    else:\n",
        "        sig_df = None\n",
        "\n",
        "    return WCT_df, aWCT_df, coi_df, sig_df"
      ],
      "metadata": {
        "id": "aIjQBUIUfWlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_scale_for_wc_mc(args):\n",
        "    s, R2, nbins = args\n",
        "    cd = np.floor(np.clip(R2[s,:], 0, 1 - np.finfo(float).eps) * nbins).astype(int)\n",
        "    wlc_local_s = np.zeros(nbins)\n",
        "    np.add.at(wlc_local_s[:], cd[~cd.mask], 1)\n",
        "    return wlc_local_s"
      ],
      "metadata": {
        "id": "2msMKkLlYmIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wct_monte_carlo_simulation(args):\n",
        "    N, al1, al2, dt, dj, s0, J, wavelet_type, nbins, maxscale, outsidecoi,scales = args\n",
        "\n",
        "    # Generates two red-noise signals with lag-1 autoregressive\n",
        "    # coefficients given by al1 and al2\n",
        "    noise1 = wavelet.rednoise(N, al1, 1)\n",
        "    noise2 = wavelet.rednoise(N, al2, 1)\n",
        "\n",
        "    # Calculate the cross wavelet transform of both red-noise signals\n",
        "    kwargs = dict(dt=dt, dj=dj, s0=s0, J=J, wavelet=wavelet_type)\n",
        "    nW1, sj, freq, coi, _, _ = wavelet.cwt(noise1, **kwargs)\n",
        "    nW2 = wavelet.cwt(noise2, **kwargs)[0]\n",
        "\n",
        "    # Delete noise variables as they are no longer needed\n",
        "    del noise1\n",
        "    del noise2\n",
        "\n",
        "    nW12 = nW1 * nW2.conj()\n",
        "\n",
        "    # Smooth wavelet wavelet transforms and calculate wavelet coherence\n",
        "    # between both signals.\n",
        "    S1 = wavelet_type.smooth(np.abs(nW1) ** 2 / scales, dt, dj, sj)\n",
        "    S2 = wavelet_type.smooth(np.abs(nW2) ** 2 / scales, dt, dj, sj)\n",
        "    S12 = wavelet_type.smooth(nW12 / scales, dt, dj, sj)\n",
        "\n",
        "    # Delete nW variables as they are no longer needed\n",
        "    del nW1\n",
        "    del nW2\n",
        "\n",
        "    R2 = np.ma.array(np.abs(S12) ** 2 / (S1 * S2), mask=~outsidecoi)\n",
        "\n",
        "    wlc_local = np.ma.zeros([J + 1,nbins])\n",
        "\n",
        "    # Walks through each scale outside the cone of influence and builds a\n",
        "    # coherence coefficient counter.\n",
        "\n",
        "    # Run the outer loop in parallel using multiple processes\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        # Split the range into batches\n",
        "        batch_size = int(maxscale/300)+1  # Adjust this value based on your memory constraints\n",
        "        batches = [range(i,i+batch_size) for i in range(0,maxscale,batch_size)]\n",
        "\n",
        "        for batch in batches:\n",
        "            results = list(executor.map(process_scale_for_wc_mc,\n",
        "                                        [(s,R2,nbins) for s in batch]))\n",
        "            for s in range(len(batch)):\n",
        "                wlc_local[batch[s],:] += results[s]\n",
        "\n",
        "            # Delete the results list to free up memory\n",
        "            del results\n",
        "\n",
        "    gc.collect()\n",
        "    return wlc_local"
      ],
      "metadata": {
        "id": "8CzpRUjYSBZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _check_parameter_wavelet(wavelet):\n",
        "    mothers = {\"morlet\": Morlet, \"paul\": Paul, \"dog\": DOG, \"mexicanhat\": MexicanHat}\n",
        "    # Checks if input parameter is a string. For backwards\n",
        "    # compatibility with Python 2 we check either if instance is a\n",
        "    # `basestring` or a `str`.\n",
        "    try:\n",
        "        if isinstance(wavelet, basestring):\n",
        "            return mothers[wavelet]()\n",
        "    except NameError:\n",
        "        if isinstance(wavelet, str):\n",
        "            return mothers[wavelet]()\n",
        "    # Otherwise, return itself.\n",
        "    return wavelet"
      ],
      "metadata": {
        "id": "pyyHWeIQS9JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wct_significance(\n",
        "    al1,\n",
        "    al2,\n",
        "    dt,\n",
        "    dj,\n",
        "    s0,\n",
        "    J,\n",
        "    significance_level=0.95,\n",
        "    wavelet_type=\"morlet\",\n",
        "    mc_count=300,\n",
        "    progress=True,\n",
        "    cache=True,\n",
        "):\n",
        "    \"\"\"Wavelet coherence transform significance.\n",
        "\n",
        "    Calculates WCT significance using Monte Carlo simulations with\n",
        "    95% confidence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    al1, al2: float\n",
        "        Lag-1 autoregressive coeficients of both time series.\n",
        "    dt : float\n",
        "        Sample spacing.\n",
        "    dj : float, optional\n",
        "        Spacing between discrete scales. Default value is 1/12.\n",
        "        Smaller values will result in better scale resolution, but\n",
        "        slower calculation and plot.\n",
        "    s0 : float, optional\n",
        "        Smallest scale of the wavelet. Default value is 2*dt.\n",
        "    J : float, optional\n",
        "        Number of scales less one. Scales range from s0 up to\n",
        "        s0 * 2**(J * dj), which gives a total of (J + 1) scales.\n",
        "        Default is J = (log2(N*dt/so))/dj.\n",
        "    significance_level : float, optional\n",
        "        Significance level to use. Default is 0.95.\n",
        "    wavelet : instance of a wavelet class, optional\n",
        "        Mother wavelet class. Default is Morlet wavelet.\n",
        "    mc_count : integer, optional\n",
        "        Number of Monte Carlo simulations. Default is 300.\n",
        "    progress : bool, optional\n",
        "        If `True` (default), shows progress bar on screen.\n",
        "    cache : bool, optional\n",
        "        If `True` (default) saves cache to file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TODO\n",
        "\n",
        "    \"\"\"\n",
        "    wavelet_type = _check_parameter_wavelet(wavelet_type)\n",
        "    if cache:\n",
        "        # Load cache if previously calculated. It is assumed that wavelet\n",
        "        # analysis is performed using the wavelet's default parameters.\n",
        "        aa = np.round(np.arctanh(np.array([al1, al2]) * 4))\n",
        "        aa = np.abs(aa) + 0.5 * (aa < 0)\n",
        "        cache_file = \"wct_sig_{:0.5f}_{:0.5f}_{:0.5f}_{:0.5f}_{:d}_{}\".format(\n",
        "            aa[0], aa[1], dj, s0 / dt, J, wavelet_type.name\n",
        "        )\n",
        "        cache_dir = get_cache_dir()\n",
        "        try:\n",
        "            dat = np.loadtxt(\"{}/{}.gz\".format(cache_dir, cache_file), unpack=True)\n",
        "            print(\"NOTE: WCT significance loaded from cache.\\n\")\n",
        "            return dat\n",
        "        except IOError:\n",
        "            pass\n",
        "\n",
        "    # Some output to the screen\n",
        "    print(\"Calculating wavelet coherence significance\")\n",
        "\n",
        "    # Choose N so that largest scale has at least some part outside the COI\n",
        "    ms = s0 * (2 ** (J * dj)) / dt\n",
        "    N = int(np.ceil(ms * 6))\n",
        "    noise1 = rednoise(N, al1, 1)\n",
        "    nW1, sj, freq, coi, _, _ = wavelet.cwt(noise1, dt=dt, dj=dj, s0=s0, J=J, wavelet=wavelet_type)\n",
        "\n",
        "    period = np.ones([1, N]) / freq[:, None]\n",
        "    coi = np.ones([J + 1, 1]) * coi[None, :]\n",
        "    outsidecoi = period <= coi\n",
        "    scales = np.ones([1, N]) * sj[:, None]\n",
        "    sig95 = np.zeros(J + 1)\n",
        "    maxscale = find(outsidecoi.any(axis=1))[-1]\n",
        "    sig95[outsidecoi.any(axis=1)] = np.nan\n",
        "\n",
        "    nbins = 1000\n",
        "    wlc = np.ma.zeros([J + 1, nbins])\n",
        "    # Displays progress bar with tqdm\n",
        "\n",
        "    # Displays progress bar with tqdm\n",
        "    with Pool(13,maxtasksperchild=1) as pool:\n",
        "      args = (N, al1, al2, dt, dj, s0, J,wavelet_type,nbins,maxscale,outsidecoi,\n",
        "              scales)\n",
        "      results = list(tqdm(pool.imap(wct_monte_carlo_simulation,[args]*mc_count,\n",
        "                                    chunksize=1),\n",
        "                          total=mc_count))\n",
        "\n",
        "    for result in results:\n",
        "        wlc += result\n",
        "\n",
        "    # After many, many, many Monte Carlo simulations, determine the\n",
        "    # significance using the coherence coefficient counter percentile.\n",
        "    wlc.mask = wlc.data == 0.0\n",
        "    R2y = (np.arange(nbins) + 0.5) / nbins\n",
        "    for s in range(maxscale):\n",
        "        sel = ~wlc[s, :].mask\n",
        "        P = wlc[s, sel].data.cumsum()\n",
        "        P = (P - 0.5) / P[-1]\n",
        "        sig95[s] = np.interp(significance_level, P, R2y[sel])\n",
        "\n",
        "    if cache:\n",
        "        # Save the results on cache to avoid to many computations in the future\n",
        "        np.savetxt(\"{}/{}.gz\".format(cache_dir, cache_file), sig95)\n",
        "\n",
        "    # And returns the results\n",
        "    return sig95\n"
      ],
      "metadata": {
        "id": "qym8d1ksM_2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wct_on_regular_chunks(df, wave1, wave2, dt=60, min_period=None, chunk_limit=40,length_limit=.25,calc_sig=False):\n",
        "    # Create a boolean series that is True where your DataFrame has NaN values\n",
        "    isna = df.isna().any(axis=1)\n",
        "\n",
        "    # Create groups for each continuous chunk of data\n",
        "    groups = isna.ne(isna.shift()).cumsum()\n",
        "\n",
        "    WCT_results = []\n",
        "    aWCT_results = []\n",
        "    coi_results = []\n",
        "    sig_results = []\n",
        "    al1_list = []\n",
        "    al2_list = []\n",
        "    for _, chunk in df.groupby(groups):\n",
        "        if not chunk.isna().any().any() and len(chunk) >= chunk_limit:\n",
        "            # Calculate min_period and max_period based on dt and the length of the chunk\n",
        "            min_period = min_period or 2 * dt\n",
        "            max_period = len(chunk) * dt * length_limit\n",
        "\n",
        "            # Run calc_wave_coherence on the chunk\n",
        "            try:\n",
        "              WCT, freq, coi, sig, aWCT, R = calc_wave_coherence(chunk[wave1], chunk[wave2], dt, min_period, max_period,sig=calc_sig)\n",
        "            except:\n",
        "              continue\n",
        "            # Convert freq to period\n",
        "            period = 1 / freq\n",
        "\n",
        "            # Create a DataFrame with timestamp as index and period as columns for WCT and aWCT\n",
        "            WCT_df = pd.DataFrame(WCT.T, index=chunk.index, columns=period)\n",
        "            aWCT_df = pd.DataFrame(aWCT.T, index=chunk.index, columns=period)\n",
        "\n",
        "            # For coi and sig, create a DataFrame with timestamp as index\n",
        "            coi_df = pd.DataFrame(coi, index=chunk.index)\n",
        "            try:\n",
        "              al1_list.append(ar1(chunk[wave1])[0])\n",
        "              al2_list.append(ar1(chunk[wave2])[0])\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "            # For sig, create a DataFrame with timestamp as index and period as columns if it's not [0]\n",
        "            if not (np.array(sig) == 0).all():\n",
        "                sig_df = pd.DataFrame(sig.T, index=chunk.index, columns=period)\n",
        "            else:\n",
        "                sig_df = None\n",
        "\n",
        "            WCT_results.append(WCT_df)\n",
        "            aWCT_results.append(aWCT_df)\n",
        "            coi_results.append(coi_df)\n",
        "            if sig_df is not None:\n",
        "                sig_results.append(sig_df)\n",
        "\n",
        "    # Combine all the results\n",
        "    combined_WCT = pd.concat(WCT_results)\n",
        "    combined_aWCT = pd.concat(aWCT_results)\n",
        "    combined_coi = pd.concat(coi_results)\n",
        "    combined_sig = pd.concat(sig_results) if sig_results else None\n",
        "    print('wave one lag 1 coef avg:',np.mean(al1_list),' std dev:',np.std(al1_list))\n",
        "    print('wave two lag 1 coef avg:',np.mean(al2_list),' std dev:',np.std(al2_list))\n",
        "    return combined_WCT, combined_aWCT, combined_coi, combined_sig\n"
      ],
      "metadata": {
        "id": "Y5UlKj47sxom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting Functions"
      ],
      "metadata": {
        "id": "YkdOVFTS0_q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###General"
      ],
      "metadata": {
        "id": "GLJ4Bef336Xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7azmtOB3wfbW"
      },
      "outputs": [],
      "source": [
        "def plot_SIF_vs_PRI(df, title, save_fig=None, filepath=None):\n",
        "    \"\"\"\n",
        "    Plot SIF versus PRI scaled for a given dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataframe containing the SIF and PRI data to plot.\n",
        "        title (str): The title of the plot.\n",
        "        save_fig (str, optional): The file path to save the figure. If None, the figure is not saved. Defaults to None.\n",
        "    \"\"\"\n",
        "    # Create a figure with multiple subplots\n",
        "    fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(10, 9), sharex=True,\n",
        "                             gridspec_kw={'height_ratios': [7, 1, 1, 1, 1, 1, 1]},\n",
        "                             squeeze=True)\n",
        "\n",
        "    # Set the color cycle for the plots\n",
        "    colors = plt.rcParams[\"axes.prop_cycle\"]()\n",
        "\n",
        "    # Plot the SIF_B_sfm data on the first subplot\n",
        "    df['SIF_B_sfm [mW m-2nm-1sr-1]'].plot(\n",
        "        legend=False,\n",
        "        ylabel='Spectral Radiance [mW m-2 nm-1 sr-1]',\n",
        "        title='SIF versus PRI scaled \\n ' + title,\n",
        "        ax=axes[0])\n",
        "\n",
        "    # Plot the SIF_A_sfm data on the first subplot\n",
        "    df['SIF_A_sfm [mW m-2nm-1sr-1]'].plot(\n",
        "        legend=False,\n",
        "        ylabel='Spectral Radiance [mW m-2 nm-1 sr-1]',\n",
        "        ax=axes[0])\n",
        "\n",
        "    # Plot the PRI_s data on the first subplot on the secondary y-axis\n",
        "    df['PRI_s'].plot(\n",
        "        secondary_y=True,\n",
        "        c='purple',\n",
        "        legend=False,\n",
        "        mark_right=True,\n",
        "        ylabel='PRI_s',\n",
        "        ax=axes[0])\n",
        "\n",
        "    # Add a legend to the first subplot\n",
        "    lines = axes[0].get_lines() + axes[0].right_ax.get_lines()\n",
        "\n",
        "    axes[0].legend(lines, [\n",
        "                       'SIF_B_sfm',\n",
        "                       'SIF_A_sfm',\n",
        "                       'PRI_s (right)',\n",
        "                       ],\n",
        "                   loc='upper left')\n",
        "\n",
        "    # Define the labels and y-axis limits for the remaining subplots\n",
        "    var_labels_list = ['PAR inc', 'PAR ref', 'APAR ', 'temp', 'humidity', 'Clearness Index']\n",
        "    y_labels_list = ['W m-2', 'W m-2', 'umol \\n m-2 s-1', 'C', '%', '']\n",
        "    tick_list = [[1, 450], [1, 55], [1, 1800], [10, 28], [1, 95], [0, .9]]\n",
        "\n",
        "    # Plot the remaining subplots\n",
        "    for i in range(len(axes[1:])):\n",
        "        c = next(colors)[\"color\"]\n",
        "\n",
        "        df.iloc[:, i + 3].plot(ax=axes[i + 1],\n",
        "                               c=c)\n",
        "\n",
        "        axes[i + 1].set_ylabel(y_labels_list[i],\n",
        "                               rotation=0)\n",
        "\n",
        "        axes[i + 1].yaxis.set_label_coords(-.05, .5)\n",
        "\n",
        "        axes[i + 1].annotate(var_labels_list[i],\n",
        "                             xy=(0.5, 0.8),\n",
        "                             xycoords='axes fraction',\n",
        "                             fontsize=8,\n",
        "                             ha=\"center\")\n",
        "\n",
        "        axes[i + 1].yaxis.tick_right()\n",
        "\n",
        "        axes[i + 1].set(ylim=tick_list[i])\n",
        "\n",
        "    # Adjust the spacing between the subplots\n",
        "    plt.subplots_adjust(hspace=0.02)\n",
        "\n",
        "    if save_fig is not None:\n",
        "      if filepath is not None:\n",
        "        fig.savefig(filepath+'SIF_vs_PRI_' + title.replace(\" \", \"_\") + '.png')\n",
        "      else:\n",
        "        fig.savefig('SIF_vs_PRI_' + title.replace(\" \", \"_\") + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx2Y7qTlNclA"
      },
      "outputs": [],
      "source": [
        "def plot_rolling_corr(df, var1, var2, win_len, resample_len, method='quantile',q=.95,save_fig=None,filepath=None,ax=None):\n",
        "    \"\"\"\n",
        "    Plot the rolling correlation between two variables in a dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataframe containing the two variables to plot.\n",
        "        var1 (str): The name of the first variable to plot.\n",
        "        var2 (str): The name of the second variable to plot.\n",
        "        win_len (str): The window length for calculating the rolling correlation. Offset string.\n",
        "        resample_len (str): The resampling frequency for calculating the 95th percentile. Offset string.\n",
        "        save_fig (str, optional): The file path to save the figure. If None, the figure is not saved. Defaults to None.\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(11, 5))\n",
        "    else:\n",
        "        fig = None\n",
        "    # Create a figure and axis\n",
        "\n",
        "    # Extract the variable names and units\n",
        "    try:\n",
        "        var1_name, var1_units = re.split(r'(?=[[])', var1)\n",
        "    except:\n",
        "        var1_name = var1\n",
        "        var1_units = var1\n",
        "    try:\n",
        "        var2_name, var2_units = re.split(r'(?=[[])', var2)\n",
        "    except:\n",
        "        var2_name = var2\n",
        "        var2_units = var2\n",
        "\n",
        "\n",
        "    # Resample the dataframe and calculate the 95th percentile\n",
        "    # Calculate the rolling correlation between the two variables\n",
        "    rolling_df = df[[var1, var2]].rolling(window=win_len, center=True)\n",
        "\n",
        "    resampled_df = df[[var1, var2]].resample(resample_len)\n",
        "    if method == 'mean':\n",
        "      resampled_df = resampled_df.mean(numeric_only=True)\n",
        "      rolling_df = rolling_df.mean(numeric_only=True)\n",
        "    elif method == 'median':\n",
        "      resampled_df = resampled_df.median(numeric_only=True)\n",
        "      rolling_df = rolling_df.median(numeric_only=True)\n",
        "    elif method == 'quantile':\n",
        "      resampled_df = resampled_df.quantile(.95,numeric_only=True)\n",
        "      rolling_df = rolling_df.quantile(.95,numeric_only=True)\n",
        "      method = method + ' ' + str(q)\n",
        "    corr_df = rolling_df[var2].rolling(window=win_len, center=True).corr(rolling_df[var1],numeric_only=True)\n",
        "    resampled_df = rolling_df\n",
        "    # Set the title of the plot\n",
        "    title_str = var1_name.replace(\"_\", \" \") + ' vs. ' + var2_name.replace(\"_\", \" \") + ' (' + win_len + ' ' + method + ')'\n",
        "\n",
        "    ax.set_title(title_str)\n",
        "\n",
        "    # Plot the first line chart\n",
        "    resampled_df[var1].plot(\n",
        "        legend=False,\n",
        "        ylabel=var1_units,\n",
        "        secondary_y=False,\n",
        "        ax=ax,\n",
        "        c='#0F6003')\n",
        "\n",
        "    # Plot the second line chart on the secondary y-axis\n",
        "    resampled_df[var2].plot(\n",
        "        c='#540360',\n",
        "        legend=False,\n",
        "        mark_right=True,\n",
        "        ylabel=var2_units,\n",
        "        secondary_y=True,\n",
        "        ax=ax)\n",
        "    ax.set_ylim(resampled_df[var1].min(), resampled_df[var1].max()) # set y-axis limits for left axis\n",
        "    ax.right_ax.set_ylim(resampled_df[var2].min(), resampled_df[var2].max()) # set y-axis limits for right axis\n",
        "\n",
        "    # Define the colors for each interval\n",
        "    colors = ['#ca0020', '#f4a582', '#f7f7f7', '#92c5de', '#0571b0'][::-1]\n",
        "\n",
        "    # Create a custom colormap from the list of colors\n",
        "    cmap = matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "    # Reclassify the correlation values into different color categories\n",
        "    coor_colors = reclassify_values_to_colors(corr_df.values,colors)\n",
        "\n",
        "    # Fill the area between the line and the x-axis with different colors based on the correlation values\n",
        "    max_value = resampled_df.max().max()\n",
        "\n",
        "    print(\"Filling in correlation background...\")\n",
        "    for color in colors:\n",
        "      start_time = time.time()\n",
        "      boolean_series = (coor_colors == color)\n",
        "\n",
        "      # Use numpy's roll function to shift the array by one element\n",
        "      shifted_series = np.roll(boolean_series, 1)\n",
        "\n",
        "      # Replace the first element with False\n",
        "      shifted_series[0] = False\n",
        "\n",
        "      ax.fill_between(x=corr_df.index,\n",
        "                    y1=0,\n",
        "                    y2=max_value,\n",
        "                    where=(boolean_series | shifted_series),\n",
        "                    facecolor=color,\n",
        "                    step='pre',\n",
        "                    alpha=.5)\n",
        "      print(color + \" time Taken: %s s\" % (time.time() - start_time))\n",
        "\n",
        "    # Add a colorbar to the plot\n",
        "    bounds = [-1, -0.6, -0.2, 0.2, 0.6, 1]\n",
        "\n",
        "    norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm,)\n",
        "\n",
        "    sm.set_array([])\n",
        "\n",
        "    cbar = plt.colorbar(sm, boundaries=bounds, ticks=bounds, ax=plt.gca())\n",
        "\n",
        "    cbar.set_label(win_len + ' Rolling Correlation')\n",
        "\n",
        "    cbar.ax.set_position([0.9, 0.15, 0.05, 0.7])\n",
        "\n",
        "    cbar.set_alpha(.5)\n",
        "\n",
        "    cbar.draw_all()\n",
        "\n",
        "    # Adjust the spacing of the plot\n",
        "    lines = ax.get_lines() + ax.right_ax.get_lines()\n",
        "\n",
        "    ax.legend(lines, [\n",
        "                   var1_name.replace(\"_\", \" \"),\n",
        "                   var2_name.replace(\"_\", \" \")\n",
        "                   ],\n",
        "               loc='upper left')\n",
        "\n",
        "    if save_fig is not None:\n",
        "      if filepath is not None:\n",
        "        fig.savefig(filepath+'rolling_corr_' + win_len + '-' + var1_name + '_' + var2_name + '_' + resample_len + '.png')\n",
        "      else:\n",
        "        fig.savefig('rolling_corr_' + win_len + '-' + var1_name + '_' + var2_name + '_' + resample_len + '.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMgDLuk_mk_X"
      },
      "outputs": [],
      "source": [
        "def plot_absorption_spectrum(radiance, date, time, df_name, save_fig=False):\n",
        "    \"\"\"\n",
        "    This function plots the absorption spectrum of the O2-A, O2-B and Ha bands for a given date and time.\n",
        "\n",
        "    Parameters:\n",
        "        radiance (pd.DataFrame): The dataframe containing the radiance values.\n",
        "        date (str): The date to plot the spectrum for, in the format 'YYYY-MM-DD'.\n",
        "        time (str): The time to plot the spectrum for, in the format 'HH:MM:SS'.\n",
        "        df_name (str): The name of the input DataFrame.\n",
        "        save_fig (bool): Whether to save the figure or not. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the wavelength ranges and names for each band\n",
        "    min_nm = ['653.','686.','754.']\n",
        "    max_nm = ['659.','692.','770.']\n",
        "    ticks = [[656.4],[687.0],[760.4]]\n",
        "    absorbtion_names = ['Ha','O2-B','O2-A']\n",
        "\n",
        "    # Create a figure and axes\n",
        "    fig,ax = plt.subplots(1,3,sharey=True)\n",
        "    fig.set_size_inches(8.5, 4.5)\n",
        "    # Loop over each band\n",
        "    for i in range(3):\n",
        "\n",
        "        # Filter the radiance dataframe by wavelength and date\n",
        "        radiance_df = radiance.T.loc[\n",
        "            radiance.T.filter(like=min_nm[i],axis=0).index[0]: #starting wl\n",
        "            radiance.T.filter(like=max_nm[i],axis=0).index[-1] #ending wl\n",
        "        ]#.loc[:,date + ' ' + time:date + ' ' + time]          #date range\n",
        "        date_radiance_df = radiance_df.loc[:,date + ' ' + time:date + ' ' + time]\n",
        "        # Plot the radiance values\n",
        "        date_radiance_df.plot(ax=ax[i],label=None,legend=False,color='darkgreen');\n",
        "\n",
        "        # Negate and flatten the radiance values\n",
        "        x = -date_radiance_df.values.flatten()\n",
        "\n",
        "        # Get the wavelength values\n",
        "        y = date_radiance_df.index.values\n",
        "\n",
        "        # Find the peaks\n",
        "        peaks, _ = find_peaks(x)\n",
        "\n",
        "        # Get the index of the maximum peak prominence\n",
        "        max_i = peak_prominences(x, peaks)[0].argmax()\n",
        "\n",
        "        # Calculate the peak widths\n",
        "        widths = peak_widths(x, peaks)\n",
        "\n",
        "        # Calculate the spectral resolution\n",
        "        spec_res = np.diff(y).mean()\n",
        "\n",
        "        # Calculate the Full Width at Half Maximum (FWHM)\n",
        "        FWHM = np.round(widths[0][max_i] * spec_res,3)\n",
        "\n",
        "        # Calculate the peak wavelength\n",
        "        peak_wl = np.round(y[peaks[max_i]],3)\n",
        "\n",
        "        # Set the axes labels and title\n",
        "        ax[i].set(xlabel='WL (nm)',ylabel='[W m-2 nm-1 sr-1]',\n",
        "                  title=absorbtion_names[i] +\n",
        "                  ', SSI: ' + str(np.round(spec_res,3)) +'nm',\n",
        "                  xticks=[float(min_nm[i]),float(max_nm[i])],)\n",
        "\n",
        "        # Add text annotations for FWHM and peak wavelength\n",
        "        ax[i].text(0.5,0.02, s='FWHM: '+str(FWHM),fontsize=10,\n",
        "                   transform=ax[i].transAxes)\n",
        "        ax[i].text(0.45,0.08, s='peak: '+str(peak_wl),fontsize=10,\n",
        "                   transform=ax[i].transAxes)\n",
        "\n",
        "        # Plot a point at the peak wavelength and value\n",
        "        ax[i].plot(peak_wl, -x[peaks[max_i]], 'ro', label='peak')\n",
        "\n",
        "    # Add a suptitle with the name of the input DataFrame, date and time\n",
        "    fig.suptitle(f'Absorption Spectrum for {df_name} on {date} at {time}')\n",
        "\n",
        "    # Add a legend outside of all three plots\n",
        "    handles, labels = ax[0].get_legend_handles_labels()\n",
        "    fig.legend(handles=handles[1:], labels=labels[1:], loc='center right')\n",
        "\n",
        "    plt.subplots_adjust(right=0.85)  # Adjust subplot to make room for legend\n",
        "\n",
        "    # Save the figure if specified\n",
        "    if save_fig:\n",
        "        fig.savefig(f'absorption_spectrum_{df_name}_{date}_{time.replace(\":\", \"-\")}.png')\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvkkF7nzeTPh"
      },
      "outputs": [],
      "source": [
        "def plot_daily_data_PAR(SIF_data_df, column_name, y_label, title, file_name):\n",
        "    \"\"\"\n",
        "    Plots daily data for each day in a calendar-like format.\n",
        "\n",
        "    Args:\n",
        "        SIF_data_df (pd.DataFrame): The input data frame containing the data to plot.\n",
        "        column_name (str): The name of the column in SIF_data_df to plot.\n",
        "        y_label (str): The label for the y-axis.\n",
        "        title (str): The title for the plot.\n",
        "        file_name (str): The name of the file to save the plot to.\n",
        "    \"\"\"\n",
        "    # Define a dictionary of colors for each rank of sunny days\n",
        "    colors = {0: 'r', 1: 'b', 2: 'g',3:'purple',4:'orange'}\n",
        "\n",
        "    # Extract the PAR daily data from the input data frame and interpolate missing values\n",
        "    PAR_daily = SIF_data_df['PAR inc [W m-2]'].copy()\n",
        "    PAR_daily.index = [PAR_daily.index.time, PAR_daily.index.date]\n",
        "    PAR_daily = PAR_daily.unstack().interpolate()\n",
        "\n",
        "    # Calculate the maximum PAR value and its string representation\n",
        "    max_par = PAR_daily.sum().max()\n",
        "    max_par_str = str(np.round(max_par/1000000,1))\n",
        "\n",
        "    # Rank the sunny days based on the total incoming PAR\n",
        "    sunny_rank = pd.cut(PAR_daily.sum(), bins=[500000,1500000,2500000,3500000,4500000,PAR_daily.sum().max()], labels=[4,3,2,1,0])\n",
        "\n",
        "    # Extract the column data from the input data frame and interpolate missing values\n",
        "    d1 = SIF_data_df[column_name].copy()\n",
        "    d1.index = [d1.index.time, d1.index.date]\n",
        "    d1 = d1.unstack().interpolate()\n",
        "\n",
        "    # Determine whether to use the sharey option based on the variance between any day\n",
        "    ranges = d1.apply(lambda col: col.max()-col.min())\n",
        "    range_ratio = ranges.max() / ranges.min()\n",
        "    sharey = range_ratio < 4\n",
        "\n",
        "    # Find the index of the first Sunday before the first day of data\n",
        "    first_date = pd.to_datetime(d1.columns[0])\n",
        "    first_sunday_index = (first_date.weekday() - 6) % 7\n",
        "\n",
        "    # Pad the data with blank days beginning on the first Sunday before the first day of data\n",
        "    blank_days = pd.DataFrame(index=d1.index, columns=pd.date_range(first_date - pd.Timedelta(days=first_sunday_index), first_date - pd.Timedelta(days=1)))\n",
        "    d1 = pd.concat([blank_days, d1], axis=1)\n",
        "\n",
        "    # Calculate the number of rows needed for the subplots\n",
        "    num_rows = int(np.ceil(len(d1.columns) / 7))\n",
        "\n",
        "    # Plot the daily data in a calendar-like format using subplots\n",
        "    axes = d1.plot(legend=0,\n",
        "            subplots=True,\n",
        "            layout=(num_rows,7),\n",
        "            figsize=(10,(10/7)*num_rows),\n",
        "            sharey=sharey,\n",
        "            sharex=True,\n",
        "            color=[colors[i] for i in sunny_rank],\n",
        "                  );\n",
        "\n",
        "    # Remove the x-ticks from all subplots\n",
        "    plt.setp(axes,\n",
        "              xticks=[],\n",
        "              )\n",
        "\n",
        "    # Define a dictionary of weekday names for each index\n",
        "    weekday_map= {0:'SUN', 1:'MON', 2:'TUE', 3:'WED',\n",
        "                  4:'THU', 5:'FRI', 6:'SAT'}\n",
        "\n",
        "    # Set the x-labels for each subplot using the weekday names\n",
        "    for i in range(7):\n",
        "      for ax in axes[:,i]:\n",
        "        ax.set_xlabel(weekday_map[i])\n",
        "\n",
        "    # Create a list of day-of-month labels for each subplot\n",
        "    day_of_month = pd.to_datetime(d1.columns).strftime('%b-%d').to_list() + [29,30,1,2,3,4,5]\n",
        "\n",
        "    # Add text annotations with the day-of-month labels above each subplot\n",
        "    for n, ax in enumerate(axes.flatten()):\n",
        "        ax.text(.50, 1.02, str(day_of_month[n]), transform=ax.transAxes,\n",
        "                size=9,\n",
        "                )\n",
        "\n",
        "        # Adjust the position of the y-tick labels to avoid overlapping with the subplot borders\n",
        "        ax.tick_params(axis='y', direction=\"in\",which='major', labelsize=8,\n",
        "                       pad=-15,\n",
        "                       )\n",
        "        ax.tick_params(axis='y', direction=\"in\",which='minor', labelsize=3,\n",
        "                       )\n",
        "\n",
        "        # Set the y-tick labels to use exponential notation if the range of y-values is large\n",
        "        y_min, y_max = ax.get_ylim()\n",
        "        if y_max - y_min > 100:\n",
        "            ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
        "        else:\n",
        "            # Define a custom formatter function to limit the number of characters in the y-tick labels\n",
        "            def y_fmt(x, pos):\n",
        "              s = f\"{x:.2g}\"\n",
        "              if len(s) > 3:\n",
        "                s = f\"{x:.1g}\"\n",
        "              return s\n",
        "            ax.yaxis.set_major_formatter(FuncFormatter(y_fmt))\n",
        "\n",
        "    # Get the figure object from the last subplot\n",
        "    fig = axes[-1,-1].get_figure()\n",
        "\n",
        "    # Add a subtitle with the total incoming PAR for the day\n",
        "    fig.suptitle('Total incoming PAR for the day (megawatt m-2)',y=0.05);\n",
        "\n",
        "    # Add a title with the input title and the date range\n",
        "    fig.text(x=0.50,y= 0.90,s=title+'\\nMay 16th - June 28th', ha='center',color='black', fontsize=14)\n",
        "\n",
        "    # Add a y-label for the whole figure\n",
        "    fig.text(0.06, 0.5,y_label , ha='center', va='center', rotation='vertical')\n",
        "\n",
        "    # Create a list of labels for the legend based on the sunny rank\n",
        "    labels = [\n",
        "        '.5 - 1.5',\n",
        "        '1.5 - 2.5',\n",
        "        '2.5 - 3.5',\n",
        "        '3.5 - 4.5',\n",
        "        '4.5 - ' + max_par_str,\n",
        "              ]\n",
        "\n",
        "    # Create a list of handles for the legend based on the colors\n",
        "    handles = [plt.Rectangle((0,0),1,1,color=colors[i]) for i in [4,3,2,1,0]]\n",
        "\n",
        "    # Add a legend below the figure with the labels and handles\n",
        "    plt.figlegend(handles, labels,\n",
        "                  loc = 'lower center',\n",
        "                  ncol=5,\n",
        "                  labelspacing=0.,\n",
        "                 );\n",
        "\n",
        "    # Adjust the position of the suptitle and legend to reduce spacing\n",
        "    fig.subplots_adjust(bottom=0.1)\n",
        "\n",
        "    # Save the figure to the input file name\n",
        "    fig.savefig(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sunny_cloudy_pairs_rolling_corr(df, var1,var2,win_len,lower=None,upper=None):\n",
        "    \"\"\"\n",
        "    This function plots the variable of interest for the day with the maximum gradient of 'CI'\n",
        "    and the following day for each month.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame. Must contain a 'CI' column and DateTimeIndex.\n",
        "    var (str): The variable of interest to plot.\n",
        "\n",
        "    Returns:\n",
        "    None. The function will plot a grid of subplots.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate daily sum, gradient, and monthly gradients\n",
        "    daily_median = df['CI'].resample('1D').median()\n",
        "    gradient = daily_median.diff().abs()\n",
        "    monthly_gradients = gradient.groupby(gradient.index.to_period('M'))\n",
        "\n",
        "    # Find the day with the maximum gradient for each month\n",
        "    max_gradient_day = monthly_gradients.idxmax()\n",
        "\n",
        "    # Initialize an empty DataFrame to store the results\n",
        "    result_df = pd.DataFrame()\n",
        "    sunny_labels=[]\n",
        "    for month, day in max_gradient_day.items():\n",
        "      # The day with the maximum gradient\n",
        "      day_data = df.loc[str(day.date())]\n",
        "      result_df = pd.concat([result_df,day_data])\n",
        "\n",
        "      # The next day\n",
        "      next_day = day - pd.DateOffset(days=1)\n",
        "      if str(next_day.date()) in df.index:  # Check if the next day exists in the original DataFrame\n",
        "        next_day_data = df.loc[str(next_day.date())]\n",
        "        result_df = pd.concat([result_df,next_day_data])\n",
        "      label1 = 'Sunny day' if next_day_data['CI'].median() > day_data['CI'].median() else 'Cloudy day'\n",
        "      label2 = 'Sunny day' if label1 == 'Cloudy day' else 'Cloudy day'\n",
        "      sunny_labels.append(label1)\n",
        "      sunny_labels.append(label2)\n",
        "\n",
        "    d1 = result_df[[var1,var2,'PAR inc [W m-2]']].copy()\n",
        "    d1.index = [d1.index.time, d1.index.date]\n",
        "    d1 = d1.unstack().interpolate()\n",
        "    num_plots = len(d1[var1].columns)# // 2\n",
        "    fig, axs = plt.subplots(num_plots, figsize=(15, 30),sharex=True)\n",
        "    day_of_month = pd.to_datetime(d1[var1].columns).strftime('%b-%d').to_list()# + [29,30,1,2,3,4,5]\n",
        "    try:\n",
        "        var1_name,_ = re.split(r'(?=[[])', var1)\n",
        "    except:\n",
        "        var1_name = var1\n",
        "    try:\n",
        "        var2_name, _ = re.split(r'(?=[[])', var2)\n",
        "    except:\n",
        "        var2_name = var2\n",
        "\n",
        "    # Define the colors for each interval\n",
        "    colors = ['#ca0020', '#f4a582', '#f7f7f7', '#92c5de', '#0571b0'][::-1]\n",
        "    # Create a custom colormap from the list of colors\n",
        "    cmap = matplotlib.colors.ListedColormap(colors)\n",
        "    corr_df = df[var1].rolling(window=win_len, center=True).corr(df[var2],numeric_only=True,min_periods=30).copy()\n",
        "    corr_df.index = [corr_df.index.time, corr_df.index.date]\n",
        "    corr_df = corr_df.unstack().interpolate()\n",
        "\n",
        "    for i in range(num_plots):\n",
        "      var1_col = d1[var1].iloc[:,i]\n",
        "      right_data = var1_col[(np.abs(stats.zscore(var1_col,nan_policy='omit')) <  3)]\n",
        "      right_data =right_data.clip(lower=0)\n",
        "      right_data.plot(\n",
        "        legend=True,\n",
        "        ylabel=var1_name,\n",
        "        label=var1_name,\n",
        "        ax=axs[i])\n",
        "      var2_col = d1[var2].iloc[:,i]\n",
        "      left_data = var2_col[(np.abs(stats.zscore(var2_col,nan_policy='omit')) <  3)]\n",
        "      left_data =left_data.clip(lower=0)\n",
        "      left_data.plot(\n",
        "        secondary_y=True,\n",
        "        c='purple',\n",
        "        legend=True,\n",
        "        mark_right=True,\n",
        "        ylabel=var2_name,\n",
        "        ax=axs[i],\n",
        "        label=var2_name)\n",
        "      axs[i].set_title(str(day_of_month[i])+' - '+sunny_labels[i])\n",
        "\n",
        "      # Fill the area between the line and the x-axis with different colors based on the correlation values\n",
        "      max_value = max(right_data.max(),left_data.max())\n",
        "\n",
        "      # Reclassify the correlation values into different color categories\n",
        "      corr_colors = reclassify_values_to_colors(corr_df.loc[:,var1_col.name].values,colors)\n",
        "      for color in colors:\n",
        "        boolean_series = (corr_colors == color)\n",
        "\n",
        "        # Use numpy's roll function to shift the array by one element\n",
        "        shifted_series = np.roll(boolean_series, 1)\n",
        "\n",
        "        # Replace the first element with False\n",
        "        shifted_series[0] = False\n",
        "        axs[i].fill_between(x=corr_df.loc[:,var1_col.name].index,\n",
        "                    y1=0,\n",
        "                    y2=max_value,\n",
        "                    where=(boolean_series | shifted_series),\n",
        "                    facecolor=color,\n",
        "                    step='post',\n",
        "                    alpha=.5)\n",
        "\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig.suptitle('Comparing Sunny and Cloudy Dirnual Patterns Across the Corn Season \\n2-minute average \\n' + 'Variables: ' +var1_name + ' vs. ' + var2_name,\n",
        "                 ha='left',x=0.02,y=1.02,fontsize=18,fontweight='bold')\n",
        "    # Add a colorbar to the plot\n",
        "    bounds = [-1, -0.6, -0.2, 0.2, 0.6, 1]\n",
        "\n",
        "    norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm,)\n",
        "\n",
        "    sm.set_array([])\n",
        "\n",
        "    cbar = plt.colorbar(sm, boundaries=bounds, ticks=bounds, ax=axs[-1],\n",
        "                        orientation='horizontal',\n",
        "                        shrink=.01)\n",
        "\n",
        "    cbar.set_label(win_len + ' Rolling Correlation',labelpad=-35,fontsize=14,fontweight='bold')\n",
        "\n",
        "    cbar.ax.set_position([0.52, 1., 0.5, 0.01])  # Adjust these values as needed\n",
        "\n",
        "    cbar.set_alpha(.5)\n",
        "\n",
        "    cbar.draw_all()\n",
        "    return"
      ],
      "metadata": {
        "id": "vEXWNk9ymYBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sunny_cloudy_pairs(df, var1,var2):\n",
        "    \"\"\"\n",
        "    This function plots the variable of interest for the day with the maximum gradient of 'CI'\n",
        "    and the following day for each month.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame. Must contain a 'CI' column and DateTimeIndex.\n",
        "    var (str): The variable of interest to plot.\n",
        "\n",
        "    Returns:\n",
        "    None. The function will plot a grid of subplots.\n",
        "    \"\"\"\n",
        "    seasonal_df = df['PRI_s'].rolling('14D').quantile(.95,numeric_only=False)\n",
        "    # Calculate daily sum, gradient, and monthly gradients\n",
        "    daily_median = df['CI'].resample('1D').median()\n",
        "    gradient = daily_median.diff().abs()\n",
        "    max_gradient_day = gradient.resample('7W').agg(\n",
        "    lambda x : np.nan if x.count() == 0 else x.idxmax()\n",
        "    )\n",
        "\n",
        "    # Initialize an empty DataFrame to store the results\n",
        "    result_df = pd.DataFrame()\n",
        "    sunny_labels=[]\n",
        "    for month, day in max_gradient_day.items():\n",
        "      # The day with the maximum gradient\n",
        "      day_data = df.loc[str(day.date())]\n",
        "      result_df = pd.concat([result_df,day_data])\n",
        "\n",
        "      # The next day\n",
        "      next_day = day - pd.DateOffset(days=1)\n",
        "      if str(next_day.date()) in df.index:  # Check if the next day exists in the original DataFrame\n",
        "        next_day_data = df.loc[str(next_day.date())]\n",
        "        result_df = pd.concat([result_df,next_day_data])\n",
        "      label1 = 'Sunny day' if next_day_data['CI'].median() > day_data['CI'].median() else 'Cloudy day'\n",
        "      label2 = 'Sunny day' if label1 == 'Cloudy day' else 'Cloudy day'\n",
        "      sunny_labels.append(label1)\n",
        "      sunny_labels.append(label2)\n",
        "\n",
        "    d1 = result_df[[var1,var2,'PAR inc [W m-2]']].copy()\n",
        "    d1.index = [d1.index.time, d1.index.date]\n",
        "    d1 = d1.unstack().interpolate()\n",
        "    num_plots = len(d1[var1].columns) // 2\n",
        "\n",
        "    fig = plt.figure(constrained_layout=True,figsize=(30,30))\n",
        "\n",
        "    day_of_month = pd.to_datetime(d1[var1].columns).strftime('%b-%d').to_list()\n",
        "\n",
        "    try:\n",
        "        var1_name,_ = re.split(r'(?=[[])', var1)\n",
        "    except:\n",
        "        var1_name = var1\n",
        "    try:\n",
        "        var2_name, _ = re.split(r'(?=[[])', var2)\n",
        "    except:\n",
        "        var2_name = var2\n",
        "\n",
        "    par_patch = mpatches.Patch(color='grey', alpha=0.5, label='PAR')\n",
        "\n",
        "\n",
        "    row_titles = ['Beginning of Season', 'Growth', 'Senescence',  'End of season',]\n",
        "\n",
        "    color_gradient = np.concatenate((np.linspace(0.6, 1, len(row_titles)//2), np.linspace(0.8, 0.6, len(row_titles)//2+1)))\n",
        "\n",
        "    colors = [plt.cm.Greens(x) for x in color_gradient]\n",
        "    subfigs = fig.subfigures(nrows=num_plots+1, ncols=1)\n",
        "\n",
        "    for row, subfig in enumerate(subfigs[1:]):\n",
        "      subfig.suptitle(row_titles[row], fontsize=26, fontweight='bold', color=colors[row])\n",
        "\n",
        "      axs = subfig.subplots(nrows=1, ncols=2)\n",
        "      for i in range(2):\n",
        "        j = row*2+i\n",
        "        left_data = d1[var1].iloc[:,j]\n",
        "        left_data =left_data.clip(upper=left_data.quantile(.95),lower=0 if abs(left_data.quantile(.05)) < .05 else left_data.quantile(.05))\n",
        "        left_data.plot(\n",
        "          legend=False,\n",
        "          ylabel=var1_name,\n",
        "          label=var1_name,\n",
        "          ax=axs[i],\n",
        "          c='C0',\n",
        "          fontsize=14)\n",
        "        right_data = d1[var2].iloc[:,j]\n",
        "        right_data =right_data.clip(upper=right_data.quantile(.95),lower=0 if abs(right_data.quantile(.05)) < .05 else right_data.quantile(.05))\n",
        "        right_data.plot(\n",
        "          secondary_y=True,\n",
        "          c='purple',\n",
        "          legend=False,\n",
        "          mark_right=True,\n",
        "          ylabel=var2_name,\n",
        "          ax=axs[i],\n",
        "          label=var2_name,\n",
        "          fontsize=14)\n",
        "\n",
        "        # Get the clipped data points for var1\n",
        "        clipped_var1 = get_clipped_values(left_data)\n",
        "\n",
        "        # Plot x markers on the left axis for var1\n",
        "        plot_clipped_markers(axs[i], clipped_var1[0][0], clipped_var1[0][1],color='black') # Below threshold\n",
        "        plot_clipped_markers(axs[i], clipped_var1[1][0], clipped_var1[1][1],color='black') # Above threshold\n",
        "\n",
        "        # Get the clipped data points for var2\n",
        "        clipped_var2 = get_clipped_values(right_data)\n",
        "\n",
        "        # Plot x markers on the right axis for var2\n",
        "        plot_clipped_markers(axs[i].right_ax, clipped_var2[0][0], clipped_var2[0][1],color='black') # Below threshold\n",
        "        plot_clipped_markers(axs[i].right_ax, clipped_var2[1][0], clipped_var2[1][1],color='black',label=None) # Above threshold\n",
        "\n",
        "        axs[i].set_title(str(day_of_month[j])+' - '+sunny_labels[j],fontsize=28,fontweight='bold')\n",
        "        par_data = d1['PAR inc [W m-2]'].iloc[:,j]\n",
        "        left_min = left_data.min()\n",
        "        left_max = left_data.max()\n",
        "        scaled_par_data = par_data / abs(par_data).max() * max(abs(left_min), abs(left_max))\n",
        "        axs[i].fill_between(x=d1.index,\n",
        "                    y1=0,\n",
        "                    y2=scaled_par_data,\n",
        "                    facecolor='grey',\n",
        "                    step='post',\n",
        "                    alpha=.5)\n",
        "        axs[i].set_ylabel(var1_name,fontsize=28)\n",
        "        axs[i].right_ax.set_ylabel(var2_name,fontsize=28)\n",
        "        # Add a dashed line to indicate the clipping threshold\n",
        "\n",
        "    ax_seasonal = subfigs[0].subplots(nrows=1, ncols=1)\n",
        "    ax_seasonal.set_position([0.05, 0.15, 0.4, 0.8])\n",
        "    seasonal_df.plot(ax=ax_seasonal,fontsize=20,c='black')\n",
        "\n",
        "    ax_seasonal.set_xlabel('Date')\n",
        "    ax_seasonal.set_ylabel('PRI - scaled and smoothed',fontsize=20)\n",
        "    subfigs[0].suptitle('Season Overview', fontsize=26, fontweight='bold',\n",
        "                        color='black',\n",
        "                        ha='left',y=.95,x=.05)\n",
        "    subfigs[0].patch.set_facecolor('none')\n",
        "\n",
        "    for i, date in enumerate(d1[var1].columns[::2]):\n",
        "\n",
        "      ax_seasonal.axvspan(date, date + pd.DateOffset(days=2), facecolor=colors[i], alpha=0.5)\n",
        "\n",
        "      ax_seasonal.annotate(row_titles[i].replace(' ','\\n'), (date, seasonal_df.min()),\n",
        "                           fontsize=28,\n",
        "                           fontweight='bold',\n",
        "                           textcoords=\"offset points\",\n",
        "                           xytext=(50,150),\n",
        "                           ha='left',\n",
        "                           color=colors[i])\n",
        "\n",
        "    #fig.legend(handles=[par_patch], labels=['Incoming PAR'],fontsize=28,)\n",
        "    # Create an empty list to store the Line2D objects\n",
        "    lines = []\n",
        "\n",
        "    lines.extend(axs[-1].get_lines())\n",
        "    lines.extend(axs[-1].right_ax.get_lines())\n",
        "    # Create labels for your lines\n",
        "    labels = [var1_name,var2_name]\n",
        "\n",
        "   # Add the par_patch to your lines and labels\n",
        "    lines.append(par_patch)\n",
        "    labels.append('Incoming PAR')\n",
        "\n",
        "    # Create a figure legend\n",
        "    fig.legend(handles=lines, labels=labels, fontsize=48,loc='upper right',bbox_to_anchor=(.9,.95),)\n",
        "\n",
        "    fig.suptitle('Comparing Sunny and Cloudy Diurnal Patterns Across the Corn Season\\n2-minute Average, Data Clipped within 5-95 Quantile For Clarity\\nVariables: ' + var1_name + ' vs. ' + var2_name,\n",
        "                 ha='center',x=.5,y=1.055,fontsize=32,fontweight='bold')\n"
      ],
      "metadata": {
        "id": "oH2J6ap6DQsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clipped_markers(axis, x_coordinates, y_coordinates, color='red', marker='x', label='Clipped'):\n",
        "    \"\"\"\n",
        "    Plots x markers on a given axis using the scatter method.\n",
        "\n",
        "    Parameters:\n",
        "    axis (matplotlib.axes.Axes): The axis to plot on.\n",
        "    x_coordinates (array-like): The x coordinates of the markers.\n",
        "    y_coordinates (array-like): The y coordinates of the markers.\n",
        "    color (str): The color of the markers. Defaults to 'red'.\n",
        "    marker (str): The marker style. Defaults to 'x'.\n",
        "    label (str): The label for the legend. Defaults to 'Clipped'.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Plot the markers using scatter\n",
        "    axis.scatter(x_coordinates, y_coordinates, c=color, marker=marker, label=label, s=4)\n"
      ],
      "metadata": {
        "id": "odVI8c1WlvdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Wavelet Coherence Plotting"
      ],
      "metadata": {
        "id": "UNA2zb4E2Qtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZcCULWDVKqf"
      },
      "outputs": [],
      "source": [
        "def calc_and_plot_wc(wave1, wave2, min_period, max_period, df,\n",
        "                     save_fig=False, scales_per_octave=12, quiv_x=2, quiv_y=20,\n",
        "                     dt=60,sig=False,cache_file=None,partial=None,wave3=None):\n",
        "    \"\"\"\n",
        "    Calculate and plot the wavelet coherence between two signals.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    wave1 : str\n",
        "        The name of the first signal in the df DataFrame.\n",
        "    wave2 : str\n",
        "        The name of the second signal in the df DataFrame.\n",
        "    min_period : float\n",
        "        The minimum period for the wavelet coherence calculation.\n",
        "    max_period : float\n",
        "        The maximum period for the wavelet coherence calculation.\n",
        "    df : pd.DataFrame\n",
        "        The DataFrame containing the signals.\n",
        "    calc_wave_coherence : function\n",
        "        The function to calculate the wavelet coherence between two signals.\n",
        "    plot_wcohere : function\n",
        "        The function to plot the wavelet coherence.\n",
        "    plot_arrows : function\n",
        "        The function to plot arrows on the wavelet coherence plot.\n",
        "    save_fig : str or None, optional\n",
        "        If not None, save the figure to a file with this name. Default is None.\n",
        "    scales_per_octave : int, optional\n",
        "        The number of voices per octave for the wavelet coherence calculation. Default is 12.\n",
        "    quiv_x : int, optional\n",
        "        The x-spacing of the arrows on the wavelet coherence plot. Default is 2.\n",
        "    quiv_y : int, optional\n",
        "        The y-spacing of the arrows on the wavelet coherence plot. Default is 20.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig : matplotlib.figure.Figure\n",
        "        The figure object containing the wavelet coherence plot.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        var1_name,_ = re.split(r'(?=[[])', wave1)\n",
        "    except:\n",
        "        var1_name = wave1\n",
        "    try:\n",
        "        var2_name, _ = re.split(r'(?=[[])', wave2)\n",
        "    except:\n",
        "        var2_name = wave2\n",
        "    try:\n",
        "        var3_name, _ = re.split(r'(?=[[])', wave3)\n",
        "    except:\n",
        "        var3_name = wave3\n",
        "\n",
        "    time_var = 'doy.dayfract'\n",
        "    wave1 = df[wave1].values\n",
        "    wave2 = df[wave2].values\n",
        "    t= df[time_var].values\n",
        "    if partial is not None:\n",
        "      if wave3 is None:\n",
        "        return print('no wave3')\n",
        "      else:\n",
        "        wave3 = df[wave3].values\n",
        "      WCT, freq, coi, sig_array, aWCT = calc_partial_wave_coherence(wave1,wave2,wave3,\n",
        "                                                    dt=dt,\n",
        "                                                    min_period=min_period,\n",
        "                                                    max_period=max_period,\n",
        "                                                    sig=sig,\n",
        "                                                    scales_per_octave=scales_per_octave,\n",
        "                                                    normalize=True)\n",
        "\n",
        "      title = \"Partial Wavelet Coherence\\n X: \" + var1_name+ ' Y: ' + var2_name + ' Z: ' + var3_name\n",
        "    else:\n",
        "      WCT, freq, coi, sig_array, aWCT,R = calc_wave_coherence(wave1, wave2,\n",
        "                                                    dt=dt,\n",
        "                                                    min_period=min_period,\n",
        "                                                    max_period=max_period,\n",
        "                                                    sig=sig,\n",
        "                                                    scales_per_octave=scales_per_octave,\n",
        "                                                    normalize=True)\n",
        "      title = \"Wavelet Coherence\\n X: \" + var1_name+ ' Y: ' + var2_name\n",
        "    #global_WCT = np.mean(WCT,axis=1)\n",
        "    #sig_global_WCT = np.mean(WCT[sig>=1],axis=1)\n",
        "    fig,ax = plt.subplots()\n",
        "    if len(sig_array) == 1:\n",
        "      sig_array = None\n",
        "    if cache_file is not None:\n",
        "      sig_array = np.loadtxt(cache_file, unpack=True)\n",
        "      sig_array = WCT/sig_array[:, None]\n",
        "    fig, [WCT, t, y_vals] = plot_wcohere(WCT,\n",
        "                                         t,\n",
        "                                         freq,\n",
        "                                         coi=coi,\n",
        "                                         sig=sig_array,\n",
        "                                         plot_period=True,\n",
        "                                         ax=ax,\n",
        "                                         title=title,\n",
        "                                         #dates=resampled_data.resample('D').mean(numeric_only=True).index.date,\n",
        "                                         )\n",
        "\n",
        "    plot_arrows(ax, [WCT, t, y_vals], aWCT=aWCT, quiv_x=quiv_x, quiv_y=quiv_y, all_arrows=False,sig=sig_array);\n",
        "\n",
        "    if save_fig:\n",
        "        plt.savefig(fig,bbox_inches='tight')\n",
        "\n",
        "    return fig,WCT, freq, coi, sig_array, aWCT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnd-DTUaN8HE"
      },
      "outputs": [],
      "source": [
        "def plot_wcohere(WCT, t, freq, coi=None, sig=None, plot_period=False, ax=None, title=\"Wavelet Coherence\", block=None, mask=None, cax=None,dates=None):\n",
        "    \"\"\"\n",
        "    Plot wavelet coherence using results from calc_wave_coherence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    *First 5 parameters can be obtained from from calc_wave_coherence\n",
        "        WCT: 2D numpy array with coherence values\n",
        "        t : 2D numpy array with sample_times\n",
        "        freq : 1D numpy array with the frequencies wavelets were calculated at\n",
        "        sig : 2D numpy array, default None\n",
        "            Optional. Plots significance of waveform coherence contours.\n",
        "        coi : 2D numpy array, default None\n",
        "            Optional. Pass coi to plot cone of influence\n",
        "    plot_period : bool\n",
        "        Should the y-axis be in period or in frequency (Hz)\n",
        "    ax : plt.axe, default None\n",
        "        Optional ax object to plot into.\n",
        "    title : str, default \"Wavelet Coherence\"\n",
        "        Optional title for the graph\n",
        "    block : [int, int]\n",
        "        Plots only points between ints.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple : (fig, wcohere_pvals)\n",
        "        Where fig is a matplotlib Figure\n",
        "        and result is a tuple consisting of [WCT, t, y_vals]\n",
        "    \"\"\"\n",
        "    dt = np.mean(np.diff(t))\n",
        "\n",
        "    if plot_period:\n",
        "        y_vals = np.log2(1 / freq)\n",
        "    if not plot_period:\n",
        "        y_vals = np.log2(freq)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    else:\n",
        "        fig = None\n",
        "\n",
        "    if mask is not None:\n",
        "        WCT = np.ma.array(WCT, mask=mask)\n",
        "\n",
        "    # Set the x and y axes of the plot\n",
        "    extent_corr = [t.min(), t.max(), 0, max(y_vals)]\n",
        "    # Fill the plot with the magnitude squared coherence values\n",
        "    im = NonUniformImage(ax, interpolation='bilinear', extent=extent_corr,cmap='jet')\n",
        "    if plot_period:\n",
        "        im.set_data(t, y_vals, WCT)\n",
        "    else:\n",
        "        im.set_data(t, y_vals[::-1], WCT[::-1, :])\n",
        "    im.set_clim(0, 1)\n",
        "    ax.add_artist(im)#ax.images.append(im)\n",
        "\n",
        "    # Plot the cone of influence - Periods greater thanthose are subject to edge effects.\n",
        "    if coi is not None:\n",
        "        # Performed by plotting a polygon\n",
        "        x_positions = np.zeros(shape=(len(t),))\n",
        "        x_positions = t\n",
        "\n",
        "        y_positions = np.zeros(shape=(len(t),))\n",
        "        if plot_period:\n",
        "            y_positions = np.log2(coi)\n",
        "        else:\n",
        "            y_positions = np.log2(1 / coi)\n",
        "\n",
        "        ax.plot(x_positions, y_positions,\n",
        "                'w--',\n",
        "                linewidth=2,\n",
        "                #c=\"w\"\n",
        "                )\n",
        "\n",
        "    # Plot the significance level contour plot\n",
        "    if sig is not None:\n",
        "        min_sig = np.min(sig[~np.isnan(sig)])\n",
        "        ax.contourf(t, y_vals,\n",
        "            sig,\n",
        "            levels=[min_sig, 1],\n",
        "            colors='grey',\n",
        "            extent=extent_corr,\n",
        "            alpha=0.5,\n",
        "            )\n",
        "        ax.contour(t, y_vals,\n",
        "                   sig,\n",
        "                   [min_sig, 1],\n",
        "                   colors='k',\n",
        "                   linewidths=1,\n",
        "                   extent=extent_corr,\n",
        "                   alpha=1,\n",
        "                   )\n",
        "\n",
        "\n",
        "    # Add limits, titles, etc.\n",
        "    ax.set_ylim(min(y_vals), max(y_vals))\n",
        "    if block:\n",
        "        ax.set_xlim(t[block[0]], t[int(block[1] * 1 / dt)])\n",
        "    else:\n",
        "        ax.set_xlim(t.min(), t.max())\n",
        "\n",
        "    if plot_period:\n",
        "        y_ticks = np.linspace(min(y_vals), max(y_vals), 8)\n",
        "        y_labels = convert_seconds_to_labels(np.exp2(y_ticks))\n",
        "        ax.set_ylabel(\"Period\")\n",
        "    else:\n",
        "        y_ticks = np.linspace(min(y_vals), max(y_vals), 8)\n",
        "        y_labels = [str(x) for x in (np.round(np.exp2(y_ticks), 3)) if x < 60 ]\n",
        "        ax.set_ylabel(\"Frequency (Hz)\")\n",
        "    ax.set_yticks(y_ticks)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    if dates is not None:\n",
        "      ax.set_xticks(np.append(np.unique(np.floor(t))[:-3:6],np.unique(np.floor(t))[-1]))\n",
        "      ax.set_xticklabels(\n",
        "          [date.strftime(\"%m-%d\") for date in dates][:-3:6]+\\\n",
        "       [[date.strftime(\"%m-%d\") for date in dates][-1]],\n",
        "                     rotation=90,\n",
        "          )\n",
        "\n",
        "    if cax is not None:\n",
        "        plt.colorbar(im, cax=cax, use_gridspec=False)\n",
        "    else:\n",
        "        if fig is not None:\n",
        "            fig.colorbar(im)\n",
        "        else:\n",
        "            plt.colorbar(im, ax=ax, use_gridspec=True)\n",
        "\n",
        "    return fig, [WCT, t, y_vals]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-ZPXrrkcG4Z"
      },
      "outputs": [],
      "source": [
        "def plot_arrows(ax, wcohere_pvals, aWCT=None, u=None, v=None, magnitude=None, quiv_x=5, quiv_y=24, all_arrows=False,sig=None):\n",
        "    \"\"\"\n",
        "    Plots phase arrows for wavelet coherence plot using results from plot_wcohere\n",
        "    source:\n",
        "    https://seankmartin.github.io/Claustrum_Experiment/html/bvmpc/lfp_coherence.html#bvmpc.lfp_coherence.plot_wave_coherence\n",
        "    Parameters\n",
        "    ----------\n",
        "    wcohere_pvals:\n",
        "        input structure, includes [WCT, t, y_vals]\n",
        "        the first three parameters are out from plot_wcohere\n",
        "    aWCT : 2D numpy\n",
        "        array with same shape as aWCT indicating phase angles\n",
        "        *Can be obtained from last value in calc_wave_coherence\n",
        "    u : 2D numpy array of unit vector's cos angle\n",
        "    v : 2D numpy array of unit vector's sin angle\n",
        "    magnitude : 2D numpy array of vector magnitude at each freq and timepoint\n",
        "    quiv_x : float\n",
        "        sets quiver window in time domain in seconds\n",
        "    quiv_y : float\n",
        "        sets number of quivers evenly distributed across freq limits\n",
        "    all_arrows : bool\n",
        "        Should phase arrows be plotted uniformly or only at high coherence\n",
        "\n",
        "    \"\"\"\n",
        "    WCT, t, y_vals = wcohere_pvals\n",
        "\n",
        "    if aWCT is not None:\n",
        "        angle = aWCT #0.5 * np.pi - aWCT  # To set zero pointing up for arrow\n",
        "        u, v = np.cos(angle), np.sin(angle)\n",
        "    elif u is None or v is None:\n",
        "        raise ValueError(\"Must pass aWCT or [u, v]\")\n",
        "\n",
        "    dt = np.mean(np.diff(t))\n",
        "\n",
        "    x_res = int(1 / dt * quiv_x)\n",
        "    y_res = int(np.ceil(len(y_vals) / quiv_y))\n",
        "    if all_arrows:\n",
        "        ax.quiver(t[::x_res], y_vals[::y_res],\n",
        "                  u[::y_res, ::x_res], v[::y_res, ::x_res], units='height',\n",
        "                  angles='uv', pivot='mid', linewidth=1, edgecolor='k', scale=30,\n",
        "                  headwidth=10, headlength=10, headaxislength=5, minshaft=2,\n",
        "                  )\n",
        "    else:\n",
        "        # t[::x_res], y_vals[::y_res],\n",
        "        # u[::y_res, ::x_res], v[::y_res, ::x_res]\n",
        "        if magnitude is not None:\n",
        "            f_mean = np.empty_like(magnitude)\n",
        "            for i, f in enumerate(magnitude):\n",
        "                # Plot arrows if magnitude > mean of particular frequency\n",
        "                f_mean[i, :] = np.mean(f) + np.std(f)\n",
        "            high_points = np.nonzero(\n",
        "                magnitude[::y_res, ::x_res] > f_mean[::y_res, ::x_res])\n",
        "\n",
        "        elif sig is not None:\n",
        "          high_points = np.nonzero(sig[::y_res, ::x_res] > 1)\n",
        "        else:\n",
        "          high_points = np.nonzero(WCT[::y_res, ::x_res] > 0.5)\n",
        "        sub_t = t[::x_res][high_points[1]]\n",
        "        sub_y = y_vals[::y_res][high_points[0]]\n",
        "        sub_u = u[::y_res, ::x_res][np.array(\n",
        "            high_points[0]), np.array(high_points[1])]\n",
        "        sub_v = v[::y_res, ::x_res][high_points[0], high_points[1]]\n",
        "        res = 1\n",
        "        ax.quiver(sub_t[::res], sub_y[::res],\n",
        "                  sub_u[::res], sub_v[::res], units='height',\n",
        "                  angles='uv', pivot='mid', linewidth=1, edgecolor='k', scale=30,\n",
        "                  headwidth=10, headlength=10, headaxislength=5, minshaft=2,\n",
        "                  )\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap(df, title='Heatmap',coi=None,sig=None):\n",
        "    df = df.copy()\n",
        "    if coi is not None:\n",
        "        coi = coi.copy()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    try:\n",
        "      df.index = df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    except:\n",
        "      print()\n",
        "\n",
        "    # Transpose the DataFrame and create the heatmap\n",
        "    ax = sns.heatmap(df.T, cmap='jet',\n",
        "                     vmin=0,vmax=1)\n",
        "\n",
        "    # Set the y-label to 'Period (minutes)'\n",
        "    ax.set_ylabel('Period (minutes)')\n",
        "\n",
        "    # Convert the y-tick labels from seconds to minutes and round to 0 decimals\n",
        "    try:\n",
        "      yticklabels = [round(float(label.get_text()) / 60, 0) for label in ax.get_yticklabels()]\n",
        "      ax.set_yticklabels(yticklabels)\n",
        "    except:\n",
        "      ax.set_ylabel('Time of Day')\n",
        "\n",
        "    # Flip the y-axis\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    # Plot the cone of influence\n",
        "    if coi is not None:\n",
        "      coi.index = coi.index.strftime('%Y-%m-%d %H:%M:%S')\n",
        "      #ax.plot(coi.index, coi.values, 'w--', linewidth=.5)\n",
        "      coi_df.plot(color='w',style='--',ax=ax)\n",
        "    if sig is not None:\n",
        "      ax.contour(sig.T>1,\n",
        "            colors='black',\n",
        "            #extent=extent_corr,\n",
        "            alpha=1,\n",
        "                 origin='lower'\n",
        "             )\n",
        "    # Rotate x-axis labels if necessary\n",
        "    #plt.xticks(rotation=45)\n",
        "\n",
        "    # Set the title\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "j57PjSvx6aQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_corr_heatmap(df, title='Heatmap',coi=None,sig=None):\n",
        "    df = df.copy()\n",
        "    if coi is not None:\n",
        "        coi = coi.copy()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Define the colors for each interval\n",
        "    colors = ['#ca0020', '#f4a582', '#f7f7f7', '#92c5de', '#0571b0'][::-1]\n",
        "\n",
        "    # Create a custom colormap from the list of colors\n",
        "    cmap = matplotlib.colors.ListedColormap(colors)\n",
        "\n",
        "    try:\n",
        "      df.index = df.index.strftime('%Y-%m-%d')\n",
        "    except:\n",
        "      print()\n",
        "\n",
        "    # Transpose the DataFrame and create the heatmap\n",
        "    ax = sns.heatmap(df.T, cmap=cmap,\n",
        "                     vmin=-1,vmax=1)\n",
        "\n",
        "    # Set the y-label to 'Period (minutes)'\n",
        "    ax.set_ylabel('Rolling Correlation Window Length (hours)')\n",
        "\n",
        "    # Convert the y-tick labels from seconds to minutes and round to 0 decimals\n",
        "    try:\n",
        "      yticklabels = [round(float(label.get_text()) / 60/60, 1) for label in ax.get_yticklabels()]\n",
        "      ax.set_yticklabels(yticklabels)\n",
        "    except:\n",
        "      ax.set_ylabel('Time of Day')\n",
        "\n",
        "    # Flip the y-axis\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "\n",
        "    # Rotate x-axis labels if necessary\n",
        "    #plt.xticks(rotation=45)\n",
        "\n",
        "    # Set the title\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vVXnTD9s2uLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_WCT_w_timeseries(df, raw_df=None, waves=None, title=\"Wavelet Coherence\", coi=None, sig=None, plot_raw_data=True):\n",
        "\n",
        "  # Sort df by its index\n",
        "  df = df.copy()\n",
        "  df.sort_index(inplace=True)\n",
        "\n",
        "  WCT = df.values.T\n",
        "  periods = df.columns.values\n",
        "\n",
        "  y_vals = np.log2(periods)\n",
        "\n",
        "  if plot_raw_data and waves is not None:\n",
        "    fig, axs = plt.subplots(3, sharex=True, figsize=(15,15), gridspec_kw={'hspace': 0})\n",
        "    ax2 = axs[1].twinx()\n",
        "    for i, wave in enumerate(waves):\n",
        "        try:\n",
        "            wave_name, wave_units = re.split(r'(?=[[])', wave)\n",
        "        except:\n",
        "            wave_name = wave\n",
        "            wave_units = wave\n",
        "        if i == 0:\n",
        "            axs[1].plot(raw_df.index, raw_df[wave], label=wave_name.replace(\"_\", \" \"))\n",
        "            axs[1].legend(loc=\"upper left\")\n",
        "        else:\n",
        "            ax2.plot(raw_df.index, raw_df[wave], label=wave_name.replace(\"_\", \" \"), color='tab:orange')\n",
        "            ax2.legend(loc=\"upper right\")\n",
        "\n",
        "  else:\n",
        "    fig, axs = plt.subplots(1, figsize=(15,5))\n",
        "    axs = [axs]\n",
        "\n",
        "  # Set the x and y axes of the plot\n",
        "  extent_corr = [mdates.date2num(df.index.min()), mdates.date2num(df.index.max()), min(y_vals), max(y_vals)]\n",
        "\n",
        "  # Fill the plot with the magnitude squared coherence values\n",
        "  im = NonUniformImage(axs[0], interpolation='bilinear', extent=extent_corr,cmap='jet')\n",
        "\n",
        "  im.set_data(mdates.date2num(df.index), y_vals, WCT)\n",
        "\n",
        "  im.set_clim(0, 1)\n",
        "\n",
        "  axs[0].add_artist(im)\n",
        "\n",
        "  # Plot the cone of influence - Periods greater than those are subject to edge effects.\n",
        "  if coi is not None:\n",
        "    # Performed by plotting a polygon\n",
        "    x_positions = mdates.date2num(coi.index)\n",
        "\n",
        "    y_positions = np.log2(coi.values.flatten())\n",
        "\n",
        "    axs[0].plot(x_positions, y_positions,\n",
        "            'w--',\n",
        "            linewidth=2,\n",
        "            )\n",
        "\n",
        "  # Plot the significance level contour plot\n",
        "  if sig is not None:\n",
        "    min_sig = np.min(sig.values[~np.isnan(sig.values)])\n",
        "\n",
        "    axs[0].contourf(mdates.date2num(df.index), y_vals,\n",
        "        sig.values,\n",
        "        levels=[min_sig, 1],\n",
        "        colors='grey',\n",
        "        extent=extent_corr,\n",
        "        alpha=0.5,\n",
        "        )\n",
        "\n",
        "    axs[0].contour(mdates.date2num(df.index), y_vals,\n",
        "               sig.values,\n",
        "               [min_sig, 1],\n",
        "               colors='k',\n",
        "               linewidths=1,\n",
        "               extent=extent_corr,\n",
        "               alpha=1,\n",
        "               )\n",
        "\n",
        "  # Add limits, titles, etc.\n",
        "  axs[0].set_ylim(min(y_vals), max(y_vals))\n",
        "  axs[0].set_xlim(mdates.date2num(df.index.min()), mdates.date2num(df.index.max()))\n",
        "\n",
        "  y_ticks = np.linspace(min(y_vals), max(y_vals), 8)\n",
        "  y_labels = convert_seconds_to_labels(np.exp2(y_ticks))\n",
        "  axs[0].set_yticks(y_ticks)\n",
        "  axs[0].set_yticklabels(y_labels)\n",
        "\n",
        "# Format x-ticks as datetime\n",
        "  axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "  axs[0].xaxis.set_major_locator(mdates.DayLocator(interval=1)) # set x-ticks to be every day\n",
        "\n",
        "# Rotate x-ticks\n",
        "  fig.autofmt_xdate()\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "A5i8Mm3zeR7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_phase_angle_histogram(phase_data: pd.DataFrame, time_lag_data: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Plot the phase angle of the wavelet coherence.\n",
        "\n",
        "    Parameters:\n",
        "    phase_data (pd.DataFrame): DataFrame containing phase data.\n",
        "    time_lag_data (pd.DataFrame): DataFrame containing time lag data with columns 'mean_angle' and 'std_angle'.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Calculate the number of rows for the plot layout\n",
        "    num_rows = int(np.ceil(len((phase_data.T * 180/np.pi).columns) / 7))\n",
        "\n",
        "    # Create subplots for each column in the DataFrame\n",
        "    axes = (phase_data.T * 180/np.pi).plot(kind='hist',subplots=True,\n",
        "                                         layout=(num_rows,7),\n",
        "                                         figsize=(10,(10/7)*num_rows),\n",
        "                                         legend=0,\n",
        "                                         sharey=True,\n",
        "                                         sharex=True,\n",
        "                                         title='Wavelet Coherence Phase Angle \\n Period in Hours'\n",
        "                                        )\n",
        "\n",
        "    # Create a list of mean and standard deviation angles\n",
        "    angle_list = [f\"{round(row['mean_angle'], 1)} +- {round(row['std_angle'], 1)}\" for _, row in time_lag_data.iterrows()]\n",
        "\n",
        "    # Create a list of period values\n",
        "    period_list = np.round(time_lag_data.index.values,2)\n",
        "\n",
        "    # Add text annotations with the day-of-month labels above each subplot\n",
        "    for n, ax in enumerate(axes.flatten()):\n",
        "        if n >= len(angle_list):\n",
        "            continue\n",
        "        ax.text(.0, 1.02, str(angle_list[n]), transform=ax.transAxes,\n",
        "                    size=9,\n",
        "                    )\n",
        "        ax.text(.0, .87, str(period_list[n]), transform=ax.transAxes,\n",
        "                    size=9,\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "02IW0OiezYmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_WCT_phase_angle_over_time(freq, coi, sig, aWCT):\n",
        "    \"\"\"\n",
        "    This function calculates and plots the phase angle based on the given parameters.\n",
        "\n",
        "    Parameters:\n",
        "    freq (numpy.ndarray): Frequency array.\n",
        "    coi (numpy.ndarray): Cone of influence array.\n",
        "    sig (numpy.ndarray): Significance array.\n",
        "    aWCT (numpy.ndarray): Cross wavelet transform array.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "\n",
        "    Sources:\n",
        "    - https://sites.google.com/a/glaciology.net/grinsted/wavelet-coherence/faq\n",
        "    - Application of the cross wavelet transform and wavelet coherence to geophysical time series\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate incoi\n",
        "    incoi = np.outer(1/freq, 1./coi) > 1\n",
        "\n",
        "    # Calculate sig_phase and sig_phase_in_coi\n",
        "    sig_phase = np.where(sig>=1,aWCT,np.nan)\n",
        "    sig_phase_in_coi = np.where(~incoi,sig_phase,np.nan)\n",
        "\n",
        "    # Create DataFrame phase_df and drop columns with all NaN values\n",
        "    phase_df = pd.DataFrame(sig_phase_in_coi)\n",
        "    phase_df.dropna(how='all',axis=1,inplace=True)\n",
        "\n",
        "    # Calculate X, Y, R, circular_mean, and circular_std\n",
        "    X = np.cos(phase_df).sum(axis=0)\n",
        "    Y = np.sin(phase_df).sum(axis=0)\n",
        "    R = np.sqrt(X**2+Y**2)\n",
        "    circular_mean = np.arctan2(Y,X)*180/np.pi\n",
        "    count = phase_df.count(axis=0)\n",
        "    circular_std = np.sqrt(-2 * np.log(R/count))*180/np.pi\n",
        "\n",
        "    # Adjust circular_mean values\n",
        "    circular_mean[circular_mean < 0] += 360\n",
        "\n",
        "    # Get x values and calculate y1 and y2 for shaded region\n",
        "    x = circular_mean.index\n",
        "    y1 = circular_mean - circular_std\n",
        "    y2 = circular_mean + circular_std\n",
        "\n",
        "    # Plot mean with shaded region for standard deviation\n",
        "    circular_mean.plot(label='Mean')\n",
        "    plt.fill_between(x, y1, y2, alpha=0.3, label='Std Dev')\n",
        "\n",
        "    # Add anti-phase line at 180 on y-axis\n",
        "    plt.axhline(180, color='red', linestyle='--', label='Anti-phase')\n",
        "\n",
        "    # Add in-phase lines at 0 and 360 on y-axis\n",
        "    plt.axhline(0, color='green', linestyle='--', label='In-phase')\n",
        "    plt.axhline(360, color='green', linestyle='--')\n",
        "\n",
        "    # Add title and legend\n",
        "    plt.title('Average Wavelet Coherence Phase Angle over Season \\n PRI_s and SIF_B ')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Add label to y-axis\n",
        "    plt.ylabel('Degrees')\n",
        "\n",
        "    plt.ylim((-5,365))\n",
        "\n"
      ],
      "metadata": {
        "id": "V3kAAuk8jF8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Animation"
      ],
      "metadata": {
        "id": "JQmwM83E2HWy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Iv0zjyqUh6k"
      },
      "outputs": [],
      "source": [
        "def animate_wavelength_range(df, min_nm, max_nm, frames, title=None, xlabel='nanometers', ylabel=None, linestyle='-', save=False, filename='animation.mp4'):\n",
        "    # filter data to specific wavelength range\n",
        "    area_of_interest_df = df.T.loc[\n",
        "        df.T.filter(like=str(min_nm)+'.', axis=0).index[0]:  # starting wl\n",
        "        df.T.filter(like=str(max_nm)+'.', axis=0).index[-1]  # ending wl\n",
        "    ].T\n",
        "\n",
        "    # create the figure and axes objects\n",
        "    fig, ax = plt.subplots()\n",
        "    scale = int(len(df) / frames)\n",
        "\n",
        "    # plot the initial data\n",
        "    line, = ax.plot(area_of_interest_df.columns, area_of_interest_df.iloc[0], linestyle=linestyle,)\n",
        "\n",
        "    # set the y-axis range limits\n",
        "    ax.set_ylim(area_of_interest_df.min().mean(), FULL_reflectance_w_PAR_df.quantile(.75).mean())\n",
        "    ax.set_xlim(min_nm, max_nm)\n",
        "    # set the x and y labels\n",
        "    if ylabel:\n",
        "        ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(xlabel)\n",
        "\n",
        "    # function to update the plot for each frame of the animation\n",
        "    def update(i):\n",
        "        # update the data of the existing plot\n",
        "        line.set_ydata(area_of_interest_df.iloc[scale * i])\n",
        "\n",
        "        # set the title to show the current minute\n",
        "        if title:\n",
        "            ax.set_title(title)\n",
        "        else:\n",
        "            ax.set_title(df.index.name + \" \" + str(area_of_interest_df.index[scale * i]))\n",
        "        return line\n",
        "    # create the animation object\n",
        "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=100,blit=False)\n",
        "\n",
        "    if save:\n",
        "        ani.save(filename)\n",
        "    else:\n",
        "        # show the animation\n",
        "        return HTML(ani.to_html5_video())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "t2tHp4knbOxU",
        "FAdfv6nbsHP3",
        "ULuE3z8dntPY",
        "eU5unpg94DqO"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}